import argparse
import logging
import os
import sys
import re
import copy  # ğŸ”¥ æ–°å¢
import torch
import torch.nn.functional as F
from pathlib import Path
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

# ğŸ”¥ è¯·ç¡®ä¿è¿™é‡Œå¯¼å…¥çš„æ˜¯ä½ æœ€æ–°çš„æ¨¡å‹å®šä¹‰æ–‡ä»¶
# å¦‚æœä½ çš„æ–‡ä»¶åæ˜¯ unet_universal3.pyï¼Œè¯·ä¿®æ”¹è¿™é‡Œ
from unet.unet_universal3 import UniversalUNet as UNet 
from utils.data_loading import BasicDataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

_EPS = 1e-6

def test_model_silent(net, device, test_loader, threshold=0.5, amp=False):
    """
    é™é»˜ç‰ˆæµ‹è¯•å‡½æ•°
    """
    net.eval()
    num_test_batches = len(test_loader)
    total_tp = 0; total_fp = 0; total_fn = 0

    with torch.no_grad():
        with torch.cuda.amp.autocast(enabled=amp):
            for batch in tqdm(test_loader, total=num_test_batches, desc='  Evaluating SWA Model', unit='batch'):
                images, true_masks = batch['image'], batch['mask']
                images = images.to(device, dtype=torch.float32, memory_format=torch.channels_last)
                true_masks = true_masks.to(device, dtype=torch.long)

                # æ¨ç†
                output = net(images)
                
                # å…¼å®¹æ€§å¤„ç†
                if isinstance(output, tuple):
                    masks_pred = output[0]
                elif isinstance(output, list):
                    masks_pred = output[0]
                else:
                    masks_pred = output

                masks_pred = torch.clamp(masks_pred, min=-50, max=50)

                # äºŒåˆ†ç±»æŒ‡æ ‡è®¡ç®—
                pred_probs = torch.sigmoid(masks_pred)
                pred_binary = (pred_probs > threshold).float()
                true_binary = true_masks.float()

                p_flat = pred_binary.view(-1)
                t_flat = true_binary.view(-1)
                
                total_tp += (p_flat * t_flat).sum()
                total_fp += (p_flat * (1 - t_flat)).sum()
                total_fn += ((1 - p_flat) * t_flat).sum()

    # è®¡ç®—å…¨å±€æŒ‡æ ‡
    dice = (2 * total_tp + _EPS) / (2 * total_tp + total_fp + total_fn + _EPS)
    iou = (total_tp + _EPS) / (total_tp + total_fp + total_fn + _EPS)
    precision = (total_tp + _EPS) / (total_tp + total_fp + _EPS)
    recall = (total_tp + _EPS) / (total_tp + total_fn + _EPS)
    f1 = (2 * precision * recall + _EPS) / (precision + recall + _EPS)

    return {
        'Dice': float(dice), 
        'IoU': float(iou), 
        'F1': float(f1), 
        'Precision': float(precision), 
        'Recall': float(recall)
    }

def get_args():
    parser = argparse.ArgumentParser(description='SWA (Weight Averaging) Test')
    
    # === ğŸ”¥ æ ¸å¿ƒæ§åˆ¶å‚æ•° ===
    parser.add_argument('--checkpoint-dir', '-d', type=str, required=True, help='å­˜æ”¾ .pth çš„æ–‡ä»¶å¤¹')
    parser.add_argument('--start-epoch', type=int, default=80, help='å¹³å‡èµ·å§‹è½®æ¬¡ (åŒ…å«)')
    parser.add_argument('--end-epoch', type=int, default=100, help='å¹³å‡ç»“æŸè½®æ¬¡ (åŒ…å«)')
    parser.add_argument('--save-swa-path', type=str, default='swa_model.pth', help='ä¿å­˜å¹³å‡åæ¨¡å‹çš„è·¯å¾„')
    
    # === æ•°æ®é›†å‚æ•° ===
    parser.add_argument('--test-img-dir', type=str, default='data/test/imgs/')
    parser.add_argument('--test-mask-dir', type=str, default='data/test/masks/')
    parser.add_argument('--scale', '-s', type=float, default=1.0)
    parser.add_argument('--batch-size', '-b', type=int, default=1)
    
    # === æ¶æ„å‚æ•° (å¿…é¡»ä¸ train.py ä¸€è‡´) ===
    # æ³¨æ„ï¼šè¿™é‡Œçš„é»˜è®¤å€¼åº”ä¸ä½ è®­ç»ƒæ—¶çš„æœ€ä½³é…ç½®ä¸€è‡´
    parser.add_argument('--cnext-type', type=str, default='convnextv2_tiny')
    parser.add_argument('--classes', '-c', type=int, default=1)
    
    # === SOTA æ¨¡å—å¼€å…³ (è¯·æ ¹æ®è®­ç»ƒæ—¶çš„è®¾ç½®å¼€å¯) ===
    parser.add_argument('--use-dual-stream', action='store_true', default=False)
    parser.add_argument('--use-deep-supervision', action='store_true', default=False)
    parser.add_argument('--use-unet3p', action='store_true', default=False)
    
    # å…¶ä»–å¯èƒ½ç”¨åˆ°çš„å‚æ•° (ä¿æŒå…¼å®¹æ€§)
    parser.add_argument('--use-wgn-enhancement', action='store_true', default=False)
    parser.add_argument('--use-cafm', action='store_true', default=False)
    parser.add_argument('--use-edge-loss', action='store_true', default=False)
    parser.add_argument('--wgn-base-order', type=int, default=3)
    parser.add_argument('--wgn-orders', type=str, default=None)
    parser.add_argument('--encoder', type=str, default='cnextv2')
    parser.add_argument('--decoder', type=str, default='phd')

    return parser.parse_args()

def extract_epoch(filename):
    """ä»æ–‡ä»¶åæå– epoch æ•°å­—"""
    match = re.search(r'epoch_(\d+)', filename)
    if match:
        return int(match.group(1))
    return None

def main():
    args = get_args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. æ‰«æå¹¶ç­›é€‰ Checkpoints
    ckpt_dir = Path(args.checkpoint_dir)
    all_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.pth')]
    
    target_files = []
    for f in all_files:
        ep = extract_epoch(f)
        if ep is not None:
            if args.start_epoch <= ep <= args.end_epoch:
                target_files.append((ep, f))
    
    # æŒ‰ epoch æ’åº
    target_files.sort(key=lambda x: x[0])
    
    if not target_files:
        logging.error(f"âŒ æ²¡æœ‰åœ¨ {ckpt_dir} æ‰¾åˆ°èŒƒå›´ [{args.start_epoch}, {args.end_epoch}] å†…çš„æƒé‡æ–‡ä»¶ï¼")
        sys.exit(1)

    print(f"\nğŸ”® SWA å‡†å¤‡å¼€å§‹ï¼šå°†èåˆä»¥ä¸‹ {len(target_files)} ä¸ªæ¨¡å‹çš„æƒé‡ï¼š")
    print(f"   Range: Epoch {target_files[0][0]} -> Epoch {target_files[-1][0]}")
    
    # 2. ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šæƒé‡å¹³å‡ (åœ¨ CPU ä¸Šè¿›è¡Œä»¥èŠ‚çœæ˜¾å­˜)
    avg_state_dict = None
    count = 0

    for ep, fname in tqdm(target_files, desc="Processing Weights"):
        path = ckpt_dir / fname
        # åŠ è½½åˆ° CPU
        checkpoint = torch.load(path, map_location='cpu')
        
        # å¤„ç†ä¸åŒä¿å­˜æ ¼å¼ (æœ‰çš„åŒ…å« optimizerï¼Œæœ‰çš„ç›´æ¥æ˜¯ dict)
        if 'model_state_dict' in checkpoint:
            curr_state_dict = checkpoint['model_state_dict']
        else:
            curr_state_dict = checkpoint
            
        if avg_state_dict is None:
            # ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œç›´æ¥æ·±æ‹·è´
            avg_state_dict = copy.deepcopy(curr_state_dict)
        else:
            # åç»­æ¨¡å‹ï¼Œç´¯åŠ å‚æ•°
            for key in avg_state_dict:
                # ç¡®ä¿åªç´¯åŠ æµ®ç‚¹ç±»å‹çš„å‚æ•° (æ’é™¤ int/long ç±»å‹çš„ bufferï¼Œå¦‚æœæœ‰çš„è¯)
                if avg_state_dict[key].is_floating_point():
                    avg_state_dict[key] += curr_state_dict[key]
        
        count += 1

    # é™¤ä»¥ Nï¼Œå–å¹³å‡
    print(f"â—æ­£åœ¨è®¡ç®—å¹³å‡å€¼ (N={count})...")
    for key in avg_state_dict:
        if avg_state_dict[key].is_floating_point():
            avg_state_dict[key] = avg_state_dict[key] / count
            
    print("âœ… æƒé‡èåˆå®Œæˆï¼")

    # 3. å‡†å¤‡æ•°æ®
    try:
        test_dataset = BasicDataset(args.test_img_dir, args.test_mask_dir, args.scale)
        test_loader = DataLoader(
            test_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=max(1, os.cpu_count() // 2), pin_memory=True, drop_last=False
        )
    except Exception as e:
        logging.error(f"Dataset Error: {e}")
        sys.exit(1)

    # 4. æ„å»ºæ¨¡å‹å¹¶åŠ è½½å¹³å‡æƒé‡
    logging.info(f"ğŸ—ï¸ Building Model... Encoder: {args.cnext_type}")
    
    # WGN Orders (å…¼å®¹æ—§ä»£ç é€»è¾‘)
    wgn_orders = None
    if args.use_wgn_enhancement:
        base = args.wgn_base_order
        wgn_orders = {'layer1': (base, base-1), 'layer2': (base+1, base), 'layer3': (base+2, base+1)}

    model = UNet(
        n_classes=args.classes,
        cnext_type=args.cnext_type,
        use_deep_supervision=args.use_deep_supervision,
        use_dual_stream=args.use_dual_stream,
        use_unet3p=args.use_unet3p,
        # ä¼ å…¥å…¶ä»–å‚æ•°ä»¥é˜²æŠ¥é”™
        use_wgn_enhancement=args.use_wgn_enhancement,
        use_cafm=args.use_cafm,
        use_edge_loss=args.use_edge_loss,
        wgn_orders=wgn_orders,
        pretrained=False # æµ‹è¯•æ¨¡å¼ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼Œç›´æ¥åŠ è½½ SWA æƒé‡
    )
    
    # åŠ è½½å¹³å‡åçš„æƒé‡
    try:
        model.load_state_dict(avg_state_dict)
        logging.info("ğŸ‰ æˆåŠŸåŠ è½½ SWA æƒé‡åˆ°æ¨¡å‹ï¼")
    except Exception as e:
        logging.error(f"åŠ è½½æƒé‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„å‚æ•°æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´: {e}")
        sys.exit(1)
        
    model.to(device)

    # 5. æµ‹è¯•èåˆåçš„æ¨¡å‹
    print("\n" + "="*50)
    print("ğŸš€ å¼€å§‹æµ‹è¯• SWA æ¨¡å‹...")
    metrics = test_model_silent(model, device, test_loader, threshold=0.5, amp=False)
    
    print("-" * 50)
    print(f"ğŸ“Š SWA Final Results (Epoch {args.start_epoch}-{args.end_epoch}):")
    print(f"   Dice      : {metrics['Dice']:.4f}")
    print(f"   IoU       : {metrics['IoU']:.4f}")
    print(f"   F1-Score  : {metrics['F1']:.4f}")
    print(f"   Precision : {metrics['Precision']:.4f}")
    print(f"   Recall    : {metrics['Recall']:.4f}")
    print("=" * 50)

    # 6. ä¿å­˜èåˆæ¨¡å‹
    if args.save_swa_path:
        save_dict = {
            'model_state_dict': avg_state_dict,
            'epoch': 'swa',
            'desc': f'SWA average from epoch {args.start_epoch} to {args.end_epoch}'
        }
        torch.save(save_dict, args.save_swa_path)
        print(f"ğŸ’¾ SWA æ¨¡å‹å·²ä¿å­˜è‡³: {args.save_swa_path}")

if __name__ == '__main__':
    main()