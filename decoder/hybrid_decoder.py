"""
hybrid_decoder.py
æ”¯æŒå‚æ•°ç©¿é€ (edge_prior) çš„è§£ç å™¨
"""
import torch
import torch.nn as nn
from decoder.mamba_helper import MambaLayer2D

try: from unet.dubm_module import DUBM_Block
except ImportError: DUBM_Block = None

try:
    import sys
    sys.path.append("./ops_dcnv3")
    from modules.dcnv3 import DCNv3
    HAS_DCN = True
except ImportError: HAS_DCN = False

# StripConvBlock ä¿æŒä¸å˜ï¼Œä½†å¢åŠ å‚æ•°æ¥æ”¶
class StripConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=7, use_dcn=False):
        super().__init__()
        padding = (kernel_size - 1) // 2
        self.proj = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))
        self.use_dcn = use_dcn and HAS_DCN
        if self.use_dcn:
            dcn_group = 4 
            self.strip_h = DCNv3(channels=out_channels, kernel_size=(1, kernel_size), stride=1, pad=(0, padding), group=dcn_group, offset_scale=1.0)
            self.strip_v = DCNv3(channels=out_channels, kernel_size=(kernel_size, 1), stride=1, pad=(padding, 0), group=dcn_group, offset_scale=1.0)
            self.norm_h = nn.BatchNorm2d(out_channels); self.norm_v = nn.BatchNorm2d(out_channels); self.act = nn.ReLU(inplace=True)
        else:
            self.strip_h = nn.Sequential(nn.Conv2d(out_channels, out_channels, (1, kernel_size), padding=(0, padding), groups=out_channels, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))
            self.strip_v = nn.Sequential(nn.Conv2d(out_channels, out_channels, (kernel_size, 1), padding=(padding, 0), groups=out_channels, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))
        self.fusion_conv = nn.Conv2d(out_channels, out_channels, 1)

    # ğŸ”¥ æ¥æ”¶ edge_prior ä½†å¿½ç•¥å®ƒï¼Œä¿è¯æ¥å£ç»Ÿä¸€
    def forward(self, x, edge_prior=None):
        x = self.proj(x)
        if self.use_dcn:
            h = self.act(self.norm_h(self.strip_h(x)))
            v = self.act(self.norm_v(self.strip_v(x)))
        else:
            h = self.strip_h(x); v = self.strip_v(x)
        return self.fusion_conv(h + v)
class OmniMambaBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # 1. é€šé“å¯¹é½å±‚ (Channel Alignment)
        # å¦‚æœè¾“å…¥è¾“å‡ºé€šé“ä¸åŒï¼Œå¿…é¡»å…ˆå¯¹é½ï¼Œå¦åˆ™æ— æ³•åšæ®‹å·®ç›¸åŠ ã€‚
        # å¦‚æœé€šé“ç›¸åŒï¼Œæ ‡å‡†çš„ Block é€šå¸¸ä¸åœ¨è¿™é‡Œå†åŠ å·ç§¯ï¼Œç›´æ¥è¿›æ®‹å·®ã€‚
        if in_channels != out_channels:
            self.align = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.align = nn.Identity() # é€šé“ä¸€è‡´æ—¶ï¼Œç›´æ¥é€ä¼ ï¼Œä¸åšå¤šä½™è®¡ç®—

        # 2. æ ¸å¿ƒ Mamba å±‚
        # MambaLayer2D å†…éƒ¨åŒ…å«äº†: Norm -> Linear(å‡ç»´) -> Conv1d -> SSM -> Linear(é™ç»´)
        self.core_op = MambaLayer2D(out_channels)
        
    def forward(self, x):
        # === æ­¥éª¤ 1: ç»´åº¦å¯¹é½ ===
        x = self.align(x)
        
        # === æ­¥éª¤ 2: å­˜å‚¨æ®‹å·® (Shortcut) ===
        # è¿™æ˜¯â€œæ ‡å‡†ç‰ˆâ€çš„çµé­‚ï¼šä¿ç•™åŸå§‹ä¿¡æ¯
        residual = x 

        # === æ­¥éª¤ 3: Mamba æ ¸å¿ƒå¤„ç† ===
        # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦å†åš self.norm(x)ï¼Œå› ä¸º core_op å†…éƒ¨ç¬¬ä¸€æ­¥å°±æ˜¯ LayerNorm (Pre-Norm ç»“æ„)
        
        # 3.1 æ­£å‘
        x1 = self.core_op(x)
        
        # 3.2 æ°´å¹³ç¿»è½¬ (æ¨¡ä»¿ä»å³åˆ°å·¦æ‰«æ)
        x2 = torch.flip(self.core_op(torch.flip(x, dims=[2, 3])), dims=[2, 3])
        
        # 3.3 å‚ç›´ç¿»è½¬ (æ¨¡ä»¿ä»ä¸‹åˆ°ä¸Šæ‰«æ)
        # å…ˆè½¬ç½®(H,Wäº’æ¢) -> æ­¤æ—¶çš„ flip ç›¸å½“äºåŸå›¾çš„å‚ç›´æ“ä½œ
        x3 = self.core_op(x.transpose(2, 3)).transpose(2, 3)
        
        # 3.4 åå‘å‚ç›´ (æ¨¡ä»¿ä»ä¸Šåˆ°ä¸‹æ‰«æ)
        x4 = torch.transpose(torch.flip(self.core_op(torch.flip(x.transpose(2, 3), dims=[2, 3])), dims=[2, 3]), 2, 3)
        
        # èåˆå››å‘ç»“æœ
        # æ ‡å‡† VMamba å¯èƒ½ä¼šç”¨ Linear åšèåˆï¼Œä½†æ±‚å¹³å‡ (Mean) æ˜¯æœ€ç¨³å¥ä¸”ä¸å¢åŠ å‚æ•°çš„æ ‡å‡†åšæ³•
        mamba_out = (x1 + x2 + x3 + x4) / 4.0
        
        # === æ­¥éª¤ 4: æ®‹å·®è¿æ¥ ===
        # Output = Input + Mamba(Norm(Input))
        return mamba_out + residual

# SKFusion ä¿æŒä¸å˜
class SKFusion(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        mid_channels = max(channels // reduction, 4)
        self.fc = nn.Sequential(nn.Linear(channels, mid_channels, bias=False), nn.ReLU(inplace=True), nn.Linear(mid_channels, 2 * channels, bias=False))
        self.softmax = nn.Softmax(dim=1)
    def forward(self, x_local, x_global):
        B, C, H, W = x_local.shape
        U = x_local + x_global 
        s = self.avg_pool(U).view(B, C)
        z = self.fc(s).view(B, 2, C)
        weights = self.softmax(z)
        return weights[:, 0].view(B, C, 1, 1) * x_local + weights[:, 1].view(B, C, 1, 1) * x_global

# PHD_DecoderBlock ä¿®æ”¹æ¥å£
class PHD_DecoderBlock(nn.Module):
    def __init__(self, in_channels, out_channels, use_dcn=False, use_dubm=False):
        super().__init__()
        self.reduce = nn.Conv2d(in_channels, out_channels, 1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.use_dubm = use_dubm and (DUBM_Block is not None)
        
        if self.use_dubm: self.local_branch = DUBM_Block(out_channels)
        elif use_dcn: self.local_branch = StripConvBlock(out_channels, out_channels, use_dcn=True)
        else: self.local_branch = StripConvBlock(out_channels, out_channels, use_dcn=False)
        
        self.global_branch = OmniMambaBlock(out_channels, out_channels)
        self.fusion = SKFusion(out_channels)

    # ğŸ”¥ å…³é”®ï¼šæ¥æ”¶å¹¶ä¼ é€’ edge_prior
    def forward(self, x, edge_prior=None):
        x = self.relu(self.bn(self.reduce(x)))
        
        if self.use_dubm:
            # åªæœ‰ D-UBM çœŸæ­£ä½¿ç”¨è¿™ä¸ªå‚æ•°
            feat_local, _ = self.local_branch(x, edge_prior=edge_prior)
        else:
            feat_local = self.local_branch(x, edge_prior=edge_prior) # å…¶ä»–æ¨¡å¼ä¼šå¿½ç•¥
            
        feat_global = self.global_branch(x)
        return self.fusion(feat_local, feat_global)
        # ================================================================
# 5. [æ–°å¢] VisualStateSpaceBlock 
# (è¿™æ˜¯ä¸ºäº†é€‚é… wvm_unet.py çš„è°ƒç”¨æ¥å£)
# ================================================================
class VisualStateSpaceBlock(nn.Module):
    """
    WVM æ¨¡å‹éœ€è¦çš„æ¥å£åŒ…è£…å™¨ã€‚
    å®ƒæ¥æ”¶ 'dim' å‚æ•°ï¼Œå¹¶åœ¨å†…éƒ¨è°ƒç”¨ OmniMambaBlockã€‚
    """
    def __init__(self, dim):
        super().__init__()
        # WVM ä¼ å…¥çš„æ˜¯ dim (ä¾‹å¦‚ 256)ï¼ŒOmniMambaBlock æ¥æ”¶ in/out channels
        self.block = OmniMambaBlock(in_channels=dim, out_channels=dim)

    def forward(self, x):
        return self.block(x)