"""
decoder/phd_decoder_pro.py
[PHD Decoder Pro] å¢å¼ºç‰ˆæ··åˆè§£ç å™¨
ç‰¹æ€§:
  1. Inverted Bottleneck (å€’æ®‹å·®): å…ˆå‡ç»´(4x)å†å¤„ç†ï¼Œå¤§å¹…å¢åŠ å‚æ•°é‡å’Œç‰¹å¾å®¹é‡ã€‚
  2. FFN (Feed-Forward Network): å¼•å…¥ä¸¤å±‚æ„ŸçŸ¥æœºï¼Œå¢å¼ºéçº¿æ€§å˜æ¢èƒ½åŠ›ã€‚
  3. Residual Connections: å†…éƒ¨å¤šé‡æ®‹å·®ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œæ”¯æŒæ·±å±‚å †å ã€‚
  4. åŒ…å«æ‰€æœ‰ä¾èµ–ç»„ä»¶ (Mamba, StripConv, SK-Fusion)ï¼Œæ— éœ€é¢å¤– importã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# ================================================================
# 0. åŸºç¡€ä¾èµ–ç»„ä»¶ (Mamba & DCN)
# ================================================================

# --- A. Mamba ç¯å¢ƒæ£€æŸ¥ ---
try:
    from mamba_ssm import Mamba
    HAS_MAMBA = True
except ImportError:
    print("âš ï¸ Warning: mamba-ssm not found. PHD Decoder Pro will fail if Mamba is required.")
    HAS_MAMBA = False

class MambaLayer2D(nn.Module):
    """ Mamba çš„ 2D é€‚é…å°è£… """
    def __init__(self, dim, d_state=16, d_conv=4, expand=2):
        super().__init__()
        if not HAS_MAMBA:
            raise ImportError("Mamba module not found. Please install mamba-ssm.")
            
        self.mamba = Mamba(
            d_model=dim,      # è¾“å…¥é€šé“æ•°
            d_state=d_state,  # çŠ¶æ€ç»´åº¦
            d_conv=d_conv,    # å±€éƒ¨å·ç§¯å®½åº¦
            expand=expand     # æ‰©å¼ ç³»æ•°
        )
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # x: [B, C, H, W]
        B, C, H, W = x.shape
        # å¼ºåˆ¶ FP32 é˜²æ­¢ Mamba æº¢å‡º
        with torch.cuda.amp.autocast(enabled=False): 
            x = x.float()
            x_seq = x.flatten(2).transpose(1, 2) # [B, L, C]
            x_seq = self.norm(x_seq)
            x_seq = self.mamba(x_seq) 
            x_out = x_seq.transpose(1, 2).view(B, C, H, W)
        return x_out

# --- B. DCNv3 ç¯å¢ƒæ£€æŸ¥ ---
try:
    # å‡è®¾æ‚¨çš„ DCNv3 è·¯å¾„å¦‚ä¸‹ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    # from ops_dcnv3.modules.dcnv3 import DCNv3
    # æš‚æ—¶ç”¨å ä½ç¬¦ï¼Œå¦‚æœæ²¡æœ‰ DCN ä¼šè‡ªåŠ¨å›é€€åˆ° StripConv
    HAS_DCN = False 
except ImportError:
    HAS_DCN = False

# ================================================================
# 1. æ ¸å¿ƒå­æ¨¡å—
# ================================================================

# --- 1.1 Strip Conv Block (å±€éƒ¨ç»†èŠ‚æµ) ---
class StripConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=7, use_dcn=True):
        super().__init__()
        padding = (kernel_size - 1) // 2
        
        # æŠ•å½±å±‚
        self.proj = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False), 
            nn.BatchNorm2d(out_channels), 
            nn.ReLU(inplace=True)
        )
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DCN (éœ€è¦åº“æ”¯æŒ)
        self.use_dcn = use_dcn and HAS_DCN
        
        if self.use_dcn:
            # DCN å®ç° (ç•¥ï¼Œéœ€å¤–éƒ¨åº“æ”¯æŒ)
            pass 
        else:
            # Fallback: ä½¿ç”¨é•¿æ¡å½¢å·ç§¯ (Strip Convolution)
            # æ°´å¹³æ¡å·ç§¯ (1 x K)
            self.strip_h = nn.Sequential(
                nn.Conv2d(out_channels, out_channels, (1, kernel_size), padding=(0, padding), 
                          groups=out_channels, bias=False), 
                nn.BatchNorm2d(out_channels), 
                nn.ReLU(inplace=True)
            )
            # å‚ç›´æ¡å·ç§¯ (K x 1)
            self.strip_v = nn.Sequential(
                nn.Conv2d(out_channels, out_channels, (kernel_size, 1), padding=(padding, 0), 
                          groups=out_channels, bias=False), 
                nn.BatchNorm2d(out_channels), 
                nn.ReLU(inplace=True)
            )
        
        self.fusion_conv = nn.Conv2d(out_channels, out_channels, 1)

    def forward(self, x):
        x = self.proj(x)
        if self.use_dcn:
            # å ä½é€»è¾‘
            return x 
        else:
            h = self.strip_h(x)
            v = self.strip_v(x)
            return self.fusion_conv(h + v)

# --- 1.2 Omni-Mamba Block (å…¨å±€è¯­ä¹‰æµ) ---
class OmniMambaBlock(nn.Module):
    """ å››å‘æ‰«æ Mamba """
    def __init__(self, in_channels, out_channels):
        super().__init__()
        if in_channels != out_channels:
            self.align = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.align = nn.Identity()

        self.core_op = MambaLayer2D(out_channels)
        
    def forward(self, x):
        x = self.align(x)
        residual = x 
        
        # å››å‘æ‰«æ: æ­£å‘ã€åå‘ã€å‚ç›´æ­£å‘ã€å‚ç›´åå‘
        x1 = self.core_op(x)
        x2 = torch.flip(self.core_op(torch.flip(x, dims=[2, 3])), dims=[2, 3])
        x3 = self.core_op(x.transpose(2, 3)).transpose(2, 3)
        x4 = torch.transpose(torch.flip(self.core_op(torch.flip(x.transpose(2, 3), dims=[2, 3])), dims=[2, 3]), 2, 3)
        
        mamba_out = (x1 + x2 + x3 + x4) / 4.0
        return mamba_out + residual

# --- 1.3 SK-Fusion (è‡ªé€‚åº”èåˆ) ---
class SKFusion(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        mid_channels = max(channels // reduction, 4)
        self.fc = nn.Sequential(
            nn.Linear(channels, mid_channels, bias=False), 
            nn.ReLU(inplace=True), 
            nn.Linear(mid_channels, 2 * channels, bias=False)
        )
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x_local, x_global):
        B, C, H, W = x_local.shape
        U = x_local + x_global 
        s = self.avg_pool(U).view(B, C)
        z = self.fc(s).view(B, 2, C)
        weights = self.softmax(z)
        w_local = weights[:, 0].view(B, C, 1, 1)
        w_global = weights[:, 1].view(B, C, 1, 1)
        return w_local * x_local + w_global * x_global

# ================================================================
# 2. ğŸ”¥ [PRO ç‰ˆæœ¬] å¢å¼ºç‰ˆè§£ç æ¨¡å—
# ================================================================

class PHD_DecoderBlock_Pro(nn.Module):
    """
    [Pro Version] å¢é‡ç‰ˆ PHD è§£ç å—
    ç­–ç•¥: 
    1. å¼•å…¥ Expansion Factor (é»˜è®¤4å€)ï¼Œå…ˆå‡ç»´å†å¤„ç†ã€‚
    2. å¢åŠ  FFN æ¨¡å—ï¼Œå¢å¼ºéçº¿æ€§ç‰¹å¾å˜æ¢ã€‚
    3. é€‚åˆå°æ•°æ®é›†ä¸‹çš„æš´åŠ›æ¶¨ç‚¹ã€‚
    """
    def __init__(self, in_channels, out_channels, expand_ratio=4, use_dcn=True):
        super().__init__()
        
        # 1. è®¡ç®—ä¸­é—´é«˜ç»´ç©ºé—´çš„ç»´åº¦
        hidden_dim = int(out_channels * expand_ratio)
        
        # 2. é€šé“å¯¹é½ (å¦‚æœè¾“å…¥è¾“å‡ºä¸ä¸€è‡´ï¼Œå…ˆå¯¹é½åˆ° out_channels)
        self.align = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # === Stage A: æ··åˆå»ºæ¨¡ (Inverted Bottleneck) ===
        # 3. å‡ç»´æŠ•å½± (1x1 Conv) -> å˜å®½
        self.expand_conv = nn.Sequential(
            nn.Conv2d(out_channels, hidden_dim, 1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.GELU() # ä½¿ç”¨ GELU æ¿€æ´»
        )
        
        # 4. åŒæµå¤„ç† (åœ¨é«˜ç»´ç©ºé—´æ“ä½œ)
        # å±€éƒ¨æµ: Strip Conv
        self.local_branch = StripConvBlock(hidden_dim, hidden_dim, use_dcn=use_dcn)
        # å…¨å±€æµ: Omni-Mamba
        self.global_branch = OmniMambaBlock(hidden_dim, hidden_dim)
        
        # 5. èåˆ (SK-Fusion)
        self.fusion = SKFusion(hidden_dim)
        
        # 6. é™ç»´æŠ•å½± (1x1 Conv) -> å˜å›åŸå®½åº¦
        self.proj_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        
        # === Stage B: FFN å¢å¼º (Feed-Forward Network) ===
        # 7. ç±»ä¼¼äº Transformer çš„ MLP å—
        ffn_dim = out_channels * 4
        self.ffn = nn.Sequential(
            nn.Conv2d(out_channels, ffn_dim, 1, bias=False),
            nn.BatchNorm2d(ffn_dim),
            nn.GELU(),
            nn.Dropout(0.1), # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Conv2d(ffn_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        
        # ç¼©æ”¾å› å­ (å¯é€‰)
        self.gamma = nn.Parameter(torch.zeros(1)) 

    def forward(self, x):
        # 1. å¯¹é½é€šé“
        x = self.align(x)
        shortcut = x
        
        # --- Block 1: Inverted Bottleneck ---
        # å‡ç»´
        x_exp = self.expand_conv(x)
        
        # åŒæµå¤„ç†
        x_local = self.local_branch(x_exp)
        x_global = self.global_branch(x_exp)
        
        # èåˆ
        x_fused = self.fusion(x_local, x_global)
        
        # é™ç»´
        x_out = self.proj_conv(x_fused)
        
        # æ®‹å·®è¿æ¥ 1
        x = shortcut + x_out
        
        # --- Block 2: FFN ---
        # æ®‹å·®è¿æ¥ 2
        x = x + self.ffn(x)
        
        return x