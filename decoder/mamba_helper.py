import torch
import torch.nn as nn

# å°è¯•å¯¼å…¥å®˜æ–¹åº“
try:
    from mamba_ssm import Mamba
except ImportError:
    print("âŒ è¿˜æ²¡æœ‰å®‰è£… mamba-ssmï¼è¯·è¿è¡Œ: pip install causal-conv1d>=1.2.0 mamba-ssm")
    Mamba = None

class MambaLayer2D(nn.Module):
    def __init__(self, dim, d_state=16, d_conv=4, expand=2):
        super().__init__()
        if Mamba is None:
            raise ImportError("Mamba module not found.")
            
        # 1. è¿™é‡Œçš„ Mamba å°±æ˜¯ä½ ç›´æ¥è°ƒç”¨çš„å®˜æ–¹åº“
        self.mamba = Mamba(
            d_model=dim,      # è¾“å…¥é€šé“æ•°
            d_state=d_state,  # çŠ¶æ€ç»´åº¦
            d_conv=d_conv,    # å±€éƒ¨å·ç§¯å®½åº¦
            expand=expand     # æ‰©å¼ ç³»æ•°
        )
        
        # 2. LayerNorm æ˜¯ Mamba çš„æ ‡é…
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # x çš„å½¢çŠ¶æ˜¯å›¾ç‰‡æ ¼å¼: [Batch, Channel, Height, Width]
        B, C, H, W = x.shape
        # ğŸ”¥ å¼€å¯é˜²çˆ†ç›¾ï¼šå¼ºåˆ¶ FP32
        with torch.cuda.amp.autocast(enabled=False):
            
            # 1. è¿›é—¨å…ˆè½¬ FP32
            x = x.float()

            # 2. å˜å½¢ (FP32)
            x_seq = x.flatten(2).transpose(1, 2) 
            
            # 3. å½’ä¸€åŒ– + Mamba å¤„ç† (FP32) <--- è¿™é‡Œæ˜¯é‡ç‚¹ï¼
            x_seq = self.norm(x_seq)
            x_seq = self.mamba(x_seq) 
            
            # 4. å˜å›å›¾ç‰‡ (FP32)
            x_out = x_seq.transpose(1, 2).view(B, C, H, W)
        
        # å‡ºäº†ç¼©è¿›å† return
        return x_out
         
        
        return x_out