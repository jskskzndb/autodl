import os
import torch
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def average_specific_epochs(ckpt_dir, epoch_list, output_path):
    """
    å¹³å‡æŒ‡å®š Epoch åˆ—è¡¨çš„æƒé‡
    """
    avg_state_dict = {}
    valid_files = []

    print(f"ğŸ“Š [ç²¾è‹±æ¨¡å¼] å‡†å¤‡å¹³å‡ä»¥ä¸‹ {len(epoch_list)} ä¸ªåæœŸé«˜åˆ†æ¨¡å‹ (Epoch >= 77):")
    
    # 1. æ£€æŸ¥å¹¶åŠ è½½æ‰€æœ‰æ¨¡å‹
    for epoch in epoch_list:
        # ä¼˜å…ˆå¯»æ‰¾ epoch æ–‡ä»¶
        fname = f"checkpoint_epoch_{epoch}.pth"
        path = os.path.join(ckpt_dir, fname)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ Epoch 77 çš„æ–‡ä»¶æ‰¾ä¸åˆ°ï¼Œå°è¯•ç”¨ checkpoint_best.pth æ›¿ä»£
        if not os.path.exists(path):
            if epoch == 77:
                logging.warning(f"âš ï¸  æœªæ‰¾åˆ° {fname}ï¼Œå°è¯•åŠ è½½ checkpoint_best.pth...")
                path = os.path.join(ckpt_dir, "checkpoint_best.pth")
            else:
                logging.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {path}ï¼Œè·³è¿‡æ­¤ Epochã€‚")
                continue
        
        if not os.path.exists(path):
             logging.error(f"âŒ ä¾ç„¶æ— æ³•æ‰¾åˆ° Epoch {epoch} çš„æƒé‡æ–‡ä»¶ã€‚")
             continue

        print(f"   -> åŠ è½½ Epoch {epoch}: {os.path.basename(path)}")
        try:
            checkpoint = torch.load(path, map_location='cpu')
            
            # å…¼å®¹æ€§å¤„ç†
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
                
            valid_files.append(state_dict)
        except Exception as e:
            logging.error(f"âŒ åŠ è½½å¤±è´¥ {path}: {e}")

    if not valid_files:
        raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æƒé‡æ–‡ä»¶ï¼")

    print(f"   (å®é™…æˆåŠŸåŠ è½½: {len(valid_files)} / {len(epoch_list)} ä¸ª)")

    # 2. æ‰§è¡Œå¹³å‡
    print(f"\nğŸ”„ æ­£åœ¨è®¡ç®—å¹³å‡å€¼...")
    first_state = valid_files[0]
    
    # è·å–æ‰€æœ‰é”®
    keys = first_state.keys()
    
    for key in keys:
        # ä»…å¹³å‡æµ®ç‚¹æ•°å‚æ•°
        if first_state[key].is_floating_point():
            sum_param = first_state[key].clone()
            for i in range(1, len(valid_files)):
                sum_param += valid_files[i][key]
            avg_state_dict[key] = sum_param / len(valid_files)
        else:
            # éæµ®ç‚¹å‚æ•°ï¼ˆå¦‚ int64 çš„ bufferï¼‰ï¼Œä¿æŒç¬¬ä¸€ä¸ªæ¨¡å‹çš„å€¼
            avg_state_dict[key] = first_state[key]

    # 3. ä¿å­˜
    save_data = {'model_state_dict': avg_state_dict}
    torch.save(save_data, output_path)
    print(f"âœ… [æˆåŠŸ] å¹³å‡æ¨¡å‹å·²ä¿å­˜è‡³: {output_path}")

# ================= é…ç½®åŒº =================
ckpt_dir = 'data/checkpoints'
output_path = os.path.join(ckpt_dir, 'checkpoint_avg_top10_late.pth')

# ğŸ”¥ ä¸¥æ ¼ç­›é€‰çš„ Epoch >= 77 çš„ Top 10
target_epochs = [
    77,   # 0.9599 (Best)
    89,   # 0.9595
    90,   # 0.9589
    81,   # 0.9588
    84,   # 0.9587
    85,   # 0.9586
    88,   # 0.9586
    91,   # 0.9586
    94,   # 0.9586
    103   # 0.9586
]

if __name__ == "__main__":
    try:
        average_specific_epochs(ckpt_dir, target_epochs, output_path)
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")