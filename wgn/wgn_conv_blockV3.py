"""
wgn_conv_block.py - V3 (Direction-Aware Version)

Upgrades from V2:
1.  [Direction-Aware]: High-frequency components (LH, HL, HH) are processed INDEPENDENTLY.
    (No more 3C -> C compression that mixes directions).
2.  [Spatially Adaptive]: Uses Learnable Soft Thresholding (retained from V2).
3.  [Guidance]: Low-frequency guides all three high-frequency branches (retained from V2).
"""

import torch
import torch.nn as nn
from pytorch_wavelets import DWTForward, DWTInverse

# =====================================================================================
# åŸºç¡€ç»„ä»¶ 1: è½¯é˜ˆå€¼å»å™ªæ¨¡å—
# =====================================================================================
class LearnableSoftThresholding(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.threshold = nn.Parameter(torch.tensor(0.02), requires_grad=True)

    def forward(self, x):
        tau = torch.abs(self.threshold)
        return torch.sign(x) * torch.relu(torch.abs(x) - tau)

# =====================================================================================
# åŸºç¡€ç»„ä»¶ 2: GnConv (ä¿æŒåŸæ ·)
# =====================================================================================
def get_dwconv(dim, kernel, bias):
    return nn.Conv2d(dim, dim, kernel_size=kernel, padding=(kernel - 1) // 2, bias=bias, groups=dim)

class GnConv(nn.Module):
    def __init__(self, dim, order=5, s=1.0):
        super().__init__()
        self.order = order
        self.dims = [dim // 2 ** i for i in range(order)]
        self.dims.reverse()
        self.proj_in = nn.Conv2d(dim, 2 * dim, 1)
        self.dwconv = get_dwconv(sum(self.dims), 7, True)
        self.proj_out = nn.Conv2d(dim, dim, 1)
        self.pws = nn.ModuleList(
            [nn.Conv2d(self.dims[i], self.dims[i + 1], 1) for i in range(order - 1)]
        )
        self.scale = s

    def forward(self, x):
        fused_x = self.proj_in(x)
        pwa, abc = torch.split(fused_x, (self.dims[0], sum(self.dims)), dim=1)
        dw_abc = self.dwconv(abc) * self.scale
        dw_list = torch.split(dw_abc, self.dims, dim=1)
        x = pwa * dw_list[0]
        for i in range(self.order - 1):
            x = self.pws[i](x) * dw_list[i + 1]
        x = self.proj_out(x)
        return x

# =====================================================================================
# ä¸»æ¨¡å—: Wg_nConv_Block V3 (æ–¹å‘æ„ŸçŸ¥ç‰ˆ)
# =====================================================================================
class Wg_nConv_Block(nn.Module):
    def __init__(self, channels, order_low=4, order_high=3):
        super().__init__()
        
        # å°æ³¢å·¥å…·
        self.dwt = DWTForward(J=1, wave='haar', mode='zero')
        self.idwt = DWTInverse(wave='haar', mode='zero')
        
        # --- 1. ä½é¢‘è·¯å¾„ (ä¸»å¯¼) ---
        self.gnconv_low_freq = GnConv(dim=channels, order=order_low)
        
        # å¼•å¯¼æ©ç ç”Ÿæˆå™¨ (1x1 Conv + Sigmoid)
        self.guidance_conv = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=1),
            nn.Sigmoid()
        )

        # --- 2. é«˜é¢‘è·¯å¾„ (V3 æ ¸å¿ƒå‡çº§: ä¸‰è·¯ç‹¬ç«‹) ---
        # ä»¥å‰ V2 æ˜¯æŠŠ 3C å‹ç¼©æˆ C (æ··åˆäº†æ–¹å‘)ã€‚
        # ç°åœ¨ V3 æ˜¯å»ºç«‹ 3 ä¸ªç‹¬ç«‹çš„ GnConvï¼Œåˆ†åˆ«å¤„ç† LH, HL, HHã€‚
        
        # A. æ°´å¹³è¾¹ç¼˜ (LH) å¤„ç†æµ
        self.branch_lh = nn.Sequential(
            LearnableSoftThresholding(channels), # å…ˆå»å™ª
            nn.Dropout(p=0.2),  # ğŸ”¥ æ–°å¢è¿™ä¸€è¡Œ
            GnConv(dim=channels, order=order_high) # å†å¢å¼º
        )
        
        # B. å‚ç›´è¾¹ç¼˜ (HL) å¤„ç†æµ
        self.branch_hl = nn.Sequential(
            LearnableSoftThresholding(channels),
            nn.Dropout(p=0.2),  # ğŸ”¥ æ–°å¢è¿™ä¸€è¡Œ
            GnConv(dim=channels, order=order_high)
        )
        
        # C. å¯¹è§’è¾¹ç¼˜ (HH) å¤„ç†æµ
        self.branch_hh = nn.Sequential(
            LearnableSoftThresholding(channels),
            nn.Dropout(p=0.2),  # ğŸ”¥ æ–°å¢è¿™ä¸€è¡Œ
            GnConv(dim=channels, order=order_high)
        )
        
        # æœ€åçš„èåˆæŠ•å½± (3C -> 3C)
        self.high_freq_proj_out = nn.Conv2d(channels * 3, channels * 3, 1)
        
    def forward(self, x):
        identity = x
        b, c, h, w = x.shape

        # 1. å°æ³¢åˆ†è§£
        ll, high_freq_list = self.dwt(x)
        high_freq = high_freq_list[0].view(b, c * 3, h // 2, w // 2)

        # 2. ä½é¢‘å¤„ç†
        ll_enhanced = self.gnconv_low_freq(ll)
        
        # ç”Ÿæˆå¼•å¯¼ Mask (Batch, C, H/2, W/2)
        guidance_mask = self.guidance_conv(ll_enhanced)

        # 3. é«˜é¢‘å¤„ç† (V3: æ‹†åˆ† -> ç‹¬ç«‹å¤„ç† -> åˆå¹¶)
        # å°† 3C æ‹†åˆ†ä¸º LH, HL, HH (æ¯ä»½æ˜¯ C)
        lh, hl, hh = torch.chunk(high_freq, 3, dim=1)
        
        # åˆ†åˆ«è¿›å…¥å„è‡ªçš„â€œå•é—´â€è¿›è¡Œå¤„ç†
        lh_out = self.branch_lh(lh)
        hl_out = self.branch_hl(hl)
        hh_out = self.branch_hh(hh)
        
        # éƒ½åœ¨è¿™é‡Œåº”ç”¨ä½é¢‘å¼•å¯¼ (Mask å¹¿æ’­ç»™ä¸‰ä¸ªåˆ†æ”¯)
        lh_out = lh_out * guidance_mask
        hl_out = hl_out * guidance_mask
        hh_out = hh_out * guidance_mask
        
        # æ‹¼æ¥å›å» (Batch, 3C, H/2, W/2)
        high_feat_combined = torch.cat([lh_out, hl_out, hh_out], dim=1)
        
        # æœ€ç»ˆèåˆä¸€ä¸‹ç‰¹å¾
        high_feat_final = self.high_freq_proj_out(high_feat_combined)
        
        # 4. å°æ³¢é‡æ„
        high_freq_out_list = [high_feat_final.view(b, c, 3, h // 2, w // 2)]
        y = self.idwt((ll_enhanced, high_freq_out_list))
        
        # 5. è¿”å›åŒç»“æœ (é€‚é…ä½ çš„åŒæµè§£ç å™¨)
        # æ³¨æ„ï¼šè¿™é‡ŒæŠŠâ€œå¤„ç†å¥½çš„é«˜é¢‘ç‰¹å¾â€ä¼ ç»™ Edge Decoder
        return identity + y, high_feat_final

# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("Testing V3 (Direction-Aware) WGN Block...")
    x = torch.randn(2, 64, 32, 32)
    block = Wg_nConv_Block(64)
    
    out, high_feat = block(x)
    
    print(f"Input: {x.shape}")
    print(f"Fused Output: {out.shape}")
    print(f"High-Freq Feature: {high_feat.shape}")
    
    assert out.shape == x.shape
    # é«˜é¢‘ç‰¹å¾åº”è¯¥æ˜¯è¾“å…¥åˆ†è¾¨ç‡çš„ä¸€åŠï¼Œé€šé“æ•°çš„3å€
    assert high_feat.shape == (2, 64*3, 16, 16)
    
    print("âœ… V3 Upgrade Successful! LH/HL/HH are processed independently.")