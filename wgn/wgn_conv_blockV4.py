"""
wgn_conv_block_cbam.py
WGN Variant: Replacing GnConv with CBAM (Convolutional Block Attention Module).

Structure:
    Input -> [3x3 Conv Feature Extraction] -> [Channel Attention] -> [Spatial Attention] -> Output

This maintains the "Gating" capability (calculating weights) while using standard operators.
"""

import torch
import torch.nn as nn
from pytorch_wavelets import DWTForward, DWTInverse


# =====================================================================================
# åŸºç¡€ç»„ä»¶ 1: è½¯é˜ˆå€¼å»å™ªæ¨¡å— (ä¿ç•™ V3 çš„ä¼˜è‰¯ç‰¹æ€§)
# =====================================================================================
class LearnableSoftThresholding(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.threshold = nn.Parameter(torch.tensor(0.02), requires_grad=True)

    def forward(self, x):
        tau = torch.abs(self.threshold)
        return torch.sign(x) * torch.relu(torch.abs(x) - tau)


# =====================================================================================
# åŸºç¡€ç»„ä»¶ 2: CBAM æ¨¡å— (æ›¿ä»£ GnConv çš„æ ¸å¿ƒ)
# =====================================================================================

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        # å¦‚æœé€šé“æ•°å¤ªå°‘ï¼Œratioè®¾å°ä¸€ç‚¹é˜²æ­¢å‹ç¼©åˆ°0
        hidden_planes = max(in_planes // ratio, 4)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # å…±äº« MLP
        self.fc1 = nn.Conv2d(in_planes, hidden_planes, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(hidden_planes, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Avgåˆ†æ”¯
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        # Maxåˆ†æ”¯
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        # å åŠ  + Sigmoid ç”Ÿæˆé€šé“æƒé‡
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        # å‹ç¼©é€šé“ä¸º2 (Max + Avg)
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # é€šé“ç»´åº¦çš„ AvgPool å’Œ MaxPool
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        # å·ç§¯ç”Ÿæˆç©ºé—´æƒé‡
        out = self.conv1(x_cat)
        return self.sigmoid(out)


class CBAMConv(nn.Module):
    """
    æ›¿ä»£ GnConv çš„ç»„åˆæ¨¡å—ï¼š
    1. 3x3 Conv: æå–å±€éƒ¨ç‰¹å¾ (ç±»ä¼¼äº GnConv é‡Œçš„ DWConv)
    2. CBAM: è®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶è¿›è¡Œé—¨æ§ (Gating)
    """

    def __init__(self, dim, order=None):  # order å‚æ•°æ˜¯ä¸ºäº†å…¼å®¹æ¥å£ï¼Œè¿™é‡Œä¸ç”¨
        super().__init__()

        # 1. ç‰¹å¾æå– (Feature Extraction)
        # å¿…é¡»å…ˆåšå·ç§¯ï¼Œæœ‰äº†ç‰¹å¾æ‰èƒ½ç®— Attention
        self.conv = nn.Sequential(
            nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True)
        )

        # 2. é—¨æ§æœºåˆ¶ (Gating / Attention)
        self.ca = ChannelAttention(dim)
        self.sa = SpatialAttention()

    def forward(self, x):
        # å…ˆæå–ç‰¹å¾
        feat = self.conv(x)

        # åº”ç”¨é€šé“æ³¨æ„åŠ› (Channel Gating)
        # Weight * Feature
        feat = self.ca(feat) * feat

        # åº”ç”¨ç©ºé—´æ³¨æ„åŠ› (Spatial Gating)
        # Weight * Feature
        feat = self.sa(feat) * feat

        return feat


# =====================================================================================
# ä¸»æ¨¡å—: Wg_nConv_Block (é›†æˆ CBAM ç‰ˆ)
# =====================================================================================
class Wg_nConv_Block(nn.Module):
    def __init__(self, channels, order_low=4, order_high=3):
        super().__init__()

        self.dwt = DWTForward(J=1, wave='haar', mode='zero')
        self.idwt = DWTInverse(wave='haar', mode='zero')

        # --- 1. ä½é¢‘è·¯å¾„ ---
        # ä½¿ç”¨ CBAMConv æ›¿ä»£ GnConv
        self.gnconv_low_freq = CBAMConv(dim=channels)

        # å¼•å¯¼ç”Ÿæˆå™¨ (ä¿æŒ V3 çš„ LL-Guided é€»è¾‘)
        self.guidance_conv = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=1),
            nn.Sigmoid()
        )

        # --- 2. é«˜é¢‘è·¯å¾„ (ä¸‰è·¯ç‹¬ç«‹) ---

        # A. æ°´å¹³è¾¹ç¼˜ (LH)
        self.branch_lh = nn.Sequential(
            LearnableSoftThresholding(channels),
            nn.Dropout(p=0.2),
            CBAMConv(dim=channels)  # ğŸ”¥ æ›¿æ¢ä¸º CBAM
        )

        # B. å‚ç›´è¾¹ç¼˜ (HL)
        self.branch_hl = nn.Sequential(
            LearnableSoftThresholding(channels),
            nn.Dropout(p=0.2),
            CBAMConv(dim=channels)  # ğŸ”¥ æ›¿æ¢ä¸º CBAM
        )

        # C. å¯¹è§’è¾¹ç¼˜ (HH)
        self.branch_hh = nn.Sequential(
            LearnableSoftThresholding(channels),
            nn.Dropout(p=0.2),
            CBAMConv(dim=channels)  # ğŸ”¥ æ›¿æ¢ä¸º CBAM
        )

        self.high_freq_proj_out = nn.Conv2d(channels * 3, channels * 3, 1)

    def forward(self, x):
        identity = x
        b, c, h, w = x.shape

        # 1. å°æ³¢åˆ†è§£
        ll, high_freq_list = self.dwt(x)
        high_freq = high_freq_list[0].view(b, c * 3, h // 2, w // 2)

        # 2. ä½é¢‘å¤„ç†
        ll_enhanced = self.gnconv_low_freq(ll)

        # ç”Ÿæˆå¼•å¯¼ Mask
        guidance_mask = self.guidance_conv(ll_enhanced)

        # 3. é«˜é¢‘å¤„ç†
        lh, hl, hh = torch.chunk(high_freq, 3, dim=1)

        lh_out = self.branch_lh(lh)
        hl_out = self.branch_hl(hl)
        hh_out = self.branch_hh(hh)

        # åº”ç”¨ä½é¢‘å¼•å¯¼ (LL-Guided)
        # æ³¨æ„ï¼šè¿™é‡Œæ˜¯åŒé‡ Gatingï¼
        # 1. å†…éƒ¨æœ‰ CBAM åš Self-Gating
        # 2. å¤–éƒ¨æœ‰ LL åš Cross-Gating
        lh_out = lh_out * guidance_mask
        hl_out = hl_out * guidance_mask
        hh_out = hh_out * guidance_mask

        high_feat_combined = torch.cat([lh_out, hl_out, hh_out], dim=1)
        high_feat_final = self.high_freq_proj_out(high_feat_combined)

        # 4. å°æ³¢é‡æ„
        high_freq_out_list = [high_feat_final.view(b, c, 3, h // 2, w // 2)]
        y = self.idwt((ll_enhanced, high_freq_out_list))

        return identity + y, high_feat_final


if __name__ == '__main__':
    print("Testing WGN with CBAM...")
    x = torch.randn(2, 64, 32, 32)
    block = Wg_nConv_Block(64)
    out, high = block(x)
    print(f"Output shape: {out.shape}")
    print("âœ… CBAM Integration Successful!")