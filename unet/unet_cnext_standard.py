"""
unet/unet_cnext_standard.py
[Ablation Baseline] ConvNeXt V2 + Standard UNet Decoder
ç”¨é€”: ç»ˆææ¶ˆèå®éªŒ Baseline
ç»“æ„: 
  - Encoder: ConvNeXt V2 Base (Pretrained)
  - Decoder: Standard DoubleConv (Conv-BN-ReLU x2)
  - Skip: Direct Concatenation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

class DoubleConv(nn.Module):
    """
    æ ‡å‡†çš„ UNet è§£ç å•å…ƒ: (Conv3x3 -> BN -> ReLU) * 2
    """
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class Standard_Up(nn.Module):
    """
    æ ‡å‡†çš„ä¸Šé‡‡æ ·æ¨¡å—
    Upsample -> Concat -> DoubleConv
    """
    def __init__(self, in_channels, out_channels, skip_channels):
        super().__init__()
        # ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        
        # è®¡ç®—æ‹¼æ¥åçš„é€šé“æ•°
        # è¾“å…¥ç»è¿‡ä¸Šé‡‡æ ·åé€šé“ä¸å˜ï¼Œæ‹¼æ¥ä¸Š skip_channels
        concat_channels = in_channels + skip_channels
        
        # é€šè¿‡åŒå·ç§¯å°†é€šé“æ•°èåˆå¹¶é™ç»´
        self.conv = DoubleConv(concat_channels, out_channels, mid_channels=in_channels // 2)

    def forward(self, x1, x2):
        # x1: æ·±å±‚ç‰¹å¾ (Decoder Input)
        # x2: æµ…å±‚ç‰¹å¾ (Skip Connection)
        x1 = self.up(x1)
        
        # å¤„ç†å¯èƒ½çš„å°ºå¯¸ä¸åŒ¹é… (Padding)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]
        if diffX != 0 or diffY != 0:
            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                            diffY // 2, diffY - diffY // 2])
        
        # æ‹¼æ¥
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class UNet_CNext_Standard(nn.Module):
    def __init__(self, n_classes=1, cnext_type='convnextv2_base', **kwargs):
        super().__init__()
        
        # ğŸ”¥ [ä¿®å¤1] å¿…é¡»åˆå§‹åŒ– n_classesï¼Œtrain.py éœ€è¦è¯»å–å®ƒ
        self.n_classes = n_classes
        
        print(f"ğŸ§ª [Ablation Baseline] ConvNeXt + Standard UNet Decoder")
        print(f"   - Encoder: {cnext_type}")
        print(f"   - Decoder: Standard DoubleConv")
        
        # --- 1. Encoder: ConvNeXt V2 ---
        # ğŸ”¥ [ä¿®å¤2] æ”¹åä¸º 'spatial_encoder' ä»¥åŒ¹é… train.py çš„å·®åˆ†å­¦ä¹ ç‡é€»è¾‘
        self.spatial_encoder = timm.create_model(
            cnext_type, 
            pretrained=True, 
            features_only=True, 
            out_indices=(0, 1, 2, 3)
        )
        
        # è·å–é€šé“æ•° (Base: [128, 256, 512, 1024])
        dims = self.spatial_encoder.feature_info.channels()
        c1, c2, c3, c4 = dims

        # --- 2. Decoder: Standard UNet Style ---
        # Up 1: Input=1024(s4), Skip=512(s3) -> Out=512
        self.up1 = Standard_Up(c4, c3, skip_channels=c3)
        
        # Up 2: Input=512(d1), Skip=256(s2) -> Out=256
        self.up2 = Standard_Up(c3, c2, skip_channels=c2)
        
        # Up 3: Input=256(d2), Skip=128(s1) -> Out=128
        self.up3 = Standard_Up(c2, c1, skip_channels=c1)
        
        # --- 3. Final Output ---
        # ConvNeXt Stem ç¼©æ”¾äº† 4 å€ï¼Œæ‰€ä»¥æœ€åéœ€è¦ä¸Šé‡‡æ · 4 å€
        self.final_up = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)
        self.outc = nn.Conv2d(c1, n_classes, kernel_size=1)

    def forward(self, x):
        # === Encoder ===
        # ğŸ”¥ [ä¿®å¤2] è¿™é‡Œè°ƒç”¨ä¹Ÿè¦æ”¹å
        s1, s2, s3, s4 = self.spatial_encoder(x)

        # === Decoder ===
        d1 = self.up1(s4, s3)
        d2 = self.up2(d1, s2)
        d3 = self.up3(d2, s1)
        
        # === Output ===
        out = self.final_up(d3)
        logits = self.outc(out)
        
        return logits