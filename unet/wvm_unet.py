"""
wvm_unet.py
----------------------------------------------------------------
Architecture: WVM-UNet (Wavelet-Visual-Mamba UNet)
Encoder: ConvNeXt V2
Decoder: Wavelet-Visual-Mamba (WVM) Upsampler
----------------------------------------------------------------
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ Mamba æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    # å¼•å…¥ä½ ç°æœ‰çš„ StripConvBlock
    from decoder.hybrid_decoder import VisualStateSpaceBlock, StripConvBlock
except ImportError:
    print("âŒ Error: Could not import VisualStateSpaceBlock from decoder.hybrid_decoder")
    VisualStateSpaceBlock = None


class SqueezeBodyEdge(nn.Module):
    def __init__(self, inplane):
        super(SqueezeBodyEdge, self).__init__()
        self.down = nn.Sequential(
            nn.Conv2d(inplane, inplane, kernel_size=3, groups=inplane, stride=2, padding=1),
            nn.BatchNorm2d(inplane),
            nn.ReLU(inplace=True),
            nn.Conv2d(inplane, inplane, kernel_size=3, groups=inplane, stride=2, padding=1),
            nn.BatchNorm2d(inplane),
            nn.ReLU(inplace=True)
        )
        self.flow_make = nn.Conv2d(inplane * 2, 2, kernel_size=3, padding=1, bias=True)
        
        # 3. ã€æ ¸å¿ƒæ­¥éª¤ã€‘å¼ºåˆ¶é›¶åˆå§‹åŒ–
        # è®©åˆå§‹çš„å…‰æµåœºå…¨ä¸º 0ï¼Œå›¾åƒä¸å‘ç”Ÿä»»ä½•æ‰­æ›²
        nn.init.constant_(self.flow_make.weight, 0)
        nn.init.constant_(self.flow_make.bias, 0)

    def forward(self, x):
        size = x.size()[2:]
        seg_down = self.down(x)
        seg_down = F.interpolate(seg_down, size=size, mode="bilinear", align_corners=True)
        flow = self.flow_make(torch.cat([x, seg_down], dim=1))
        seg_flow_warp = self.flow_warp(x, flow, size)
        seg_edge = x - seg_flow_warp
        return seg_flow_warp, seg_edge

    def flow_warp(self, input, flow, size):
        out_h, out_w = size
        n, c, h, w = input.size()
        norm = torch.tensor([[[[out_w, out_h]]]]).type_as(input).to(input.device)
        h_grid = torch.linspace(-1.0, 1.0, out_h, device=input.device).view(-1, 1).repeat(1, out_w)
        w_grid = torch.linspace(-1.0, 1.0, out_w, device=input.device).repeat(out_h, 1)
        grid = torch.cat((w_grid.unsqueeze(2), h_grid.unsqueeze(2)), 2)
        grid = grid.repeat(n, 1, 1, 1)
        grid = grid + flow.permute(0, 2, 3, 1) / norm
        return F.grid_sample(input, grid, align_corners=True)

# å®šä¹‰ä¸€ä¸ªç”Ÿæˆæ ‡å‡†æ£€æµ‹å¤´çš„è¾…åŠ©å‡½æ•° (Conv3x3 -> Conv1x1)
def make_head(in_ch, out_ch):
    return nn.Sequential(
        nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1, bias=False),
        nn.BatchNorm2d(in_ch),
        nn.ReLU(inplace=True),
        nn.Conv2d(in_ch, out_ch, kernel_size=1)
    )

class SE_Projection(nn.Module):
    """
    å¸¦é€šé“æ³¨æ„åŠ›æœºåˆ¶çš„æŠ•å½±å±‚ (SE-Weighted Projection)
    ä½œç”¨ï¼šåœ¨å‹ç¼©é€šé“ä¹‹å‰ï¼Œå…ˆåˆ¤æ–­å“ªäº›é€šé“æ˜¯é‡è¦çš„ï¼ˆæ¯”å¦‚è¿™æ˜¯æˆ¿å­ï¼‰ï¼Œå“ªäº›æ˜¯ä¸é‡è¦çš„ï¼ˆæ¯”å¦‚è¿™æ˜¯èƒŒæ™¯ï¼‰ï¼Œ
          ç»™é‡è¦çš„é€šé“åŠ æƒï¼Œä¸é‡è¦çš„æŠ‘åˆ¶ï¼Œç„¶åå†è¿›è¡Œ 1x1 å·ç§¯å‹ç¼©ã€‚
    """
    def __init__(self, in_channels, out_channels, reduction=16):
        super().__init__()
        # 1. SE æ¨¡å— (è®¡ç®—é€šé“æƒé‡)
        # ç¡®ä¿ reduction ä¸ä¼šè®©é€šé“æ•°å˜æˆ 0ï¼Œæœ€å°ä¸º 4
        mid = max(in_channels // reduction, 4)
        
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),             # Squeeze: å…¨å±€æ± åŒ–
            nn.Conv2d(in_channels, mid, 1),      # Excitation 1: é™ç»´
            nn.ReLU(inplace=True),
            nn.Conv2d(mid, in_channels, 1),      # Excitation 2: å‡ç»´
            nn.Sigmoid()                         # è¾“å‡º 0~1 çš„æƒé‡
        )
        
        # 2. æŠ•å½±å±‚ (åŸå§‹çš„ 1x1 å·ç§¯)
        self.proj = nn.Conv2d(in_channels, out_channels, 1, bias=False)

    def forward(self, x):
        # x: [B, C, H, W]
        w = self.se(x)     # è®¡ç®—æƒé‡
        x = x * w          # é‡æ–°åŠ æƒ (Reweight)
        return self.proj(x) # æŠ•å½±å‹ç¼©
# ================================================================
# 1. Haar å°æ³¢å˜æ¢
# ================================================================
class HaarWaveletTransform(nn.Module):
    def __init__(self):
        super().__init__()

    def dwt(self, x):
        x00 = x[:, :, 0::2, 0::2]
        x01 = x[:, :, 0::2, 1::2]
        x10 = x[:, :, 1::2, 0::2]
        x11 = x[:, :, 1::2, 1::2]
        ll = (x00 + x01 + x10 + x11) / 2
        lh = (x00 + x01 - x10 - x11) / 2
        hl = (x00 - x01 + x10 - x11) / 2
        hh = (x00 - x01 - x10 + x11) / 2
        return ll, lh, hl, hh

    def idwt(self, ll, lh, hl, hh):
        x00 = (ll + lh + hl + hh) / 2
        x01 = (ll + lh - hl - hh) / 2
        x10 = (ll - lh + hl - hh) / 2
        x11 = (ll - lh - hl + hh) / 2
        b, c, h, w = ll.shape
        out = torch.zeros(b, c, h * 2, w * 2, device=ll.device, dtype=ll.dtype)
        out[:, :, 0::2, 0::2] = x00
        out[:, :, 0::2, 1::2] = x01
        out[:, :, 1::2, 0::2] = x10
        out[:, :, 1::2, 1::2] = x11
        return out


# ================================================================
# 2. WVM ä¸Šé‡‡æ ·å™¨ (æ ¸å¿ƒæ¨¡å—)
# ================================================================
class WVM_Upsampler(nn.Module):
    def __init__(self, deep_channels, skip_channels, out_channels, use_dcn=False):
        super().__init__()
        
        if VisualStateSpaceBlock is None:
            raise ImportError("Mamba module not found.")

        self.dwt_idwt = HaarWaveletTransform()
        self.mid_channels = out_channels
        
        # æŠ•å½±å±‚
        
        # ğŸ”¥æ™®é€šå·ç§¯
        self.deep_proj = nn.Conv2d(deep_channels, self.mid_channels, 1)
        
        # âœ… æ–°ä»£ç :
        #self.deep_proj = SE_Projection(deep_channels, self.mid_channels)
        
        # 2. è·³è·ƒè¿æ¥ä¿æŒæ™®é€šå·ç§¯ (ä¿ç•™åŸå§‹ç»†èŠ‚)
        self.skip_proj = nn.Conv2d(skip_channels, self.mid_channels, 1)
        
        # èåˆå±‚ (è¾“å…¥ 4 ä¸ªåˆ†é‡)
        #self.fusion_conv = nn.Conv2d(self.mid_channels * 4, self.mid_channels * 4, 1)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šç©ºé—´å¯¹é½æ¨¡å— (Strip Convolution) ğŸ”¥ğŸ”¥ğŸ”¥
        # æ”¾åœ¨èåˆä¹‹åï¼ŒMamba ä¹‹å‰
        # è¾“å…¥é€šé“æ˜¯ mid_channels * 4 (å› ä¸ºæ‹¼æ¥äº†4ä¸ªåˆ†é‡)
        #self.align_module = StripConvBlock(
        #    in_channels=self.mid_channels * 4, 
        #    out_channels=self.mid_channels * 4, 
        #    kernel_size=7, # ä½ æƒ³è¦çš„å¤§æ ¸
        #    use_dcn=use_dcn  # è®¾ä¸º True å°±æ˜¯ç”¨ DCNï¼Œè®¾ä¸º False å°±æ˜¯ç”¨ Strip Conv
        #)

        # Mamba é¢‘åŸŸç­›é€‰
        #self.mamba_selector = VisualStateSpaceBlock(dim=self.mid_channels * 4)
        
        # [ä¿®æ”¹] ç§»é™¤ fusion_convï¼Œæ”¹ä¸ºåˆ†æµå¤„ç†
        
        # [åˆ†æ”¯ A] ä½é¢‘è¯­ä¹‰ (LL) -> Mamba
        # åªå¤„ç† 1 ä¸ªåˆ†é‡ï¼Œå‚æ•°é‡å¤§å¹…é™ä½
        self.mamba_ll = VisualStateSpaceBlock(dim=self.mid_channels)

        # [åˆ†æ”¯ B] é«˜é¢‘è¾¹ç¼˜ (LH, HL, HH) -> Strip DCN
        # å¤„ç† 3 ä¸ªåˆ†é‡ï¼Œä½¿ç”¨ 1x7 å’Œ 7x1 å¹¶è¡Œå·ç§¯æ•æ‰å‡ ä½•è¾¹ç¼˜
        # å¼ºåˆ¶å¼€å¯ use_dcn=True ä»¥å¤„ç†å€¾æ–œ/ä¸è§„åˆ™è¾¹ç¼˜
        self.edge_align = StripConvBlock(
            in_channels=self.mid_channels * 3,
            out_channels=self.mid_channels * 3,
            kernel_size=7,    
            use_dcn=True      
        )


        # è¾“å‡ºå¹³æ»‘
        self.out_conv = nn.Sequential(
            nn.Conv2d(self.mid_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x_deep, x_skip):
        # 1. å‡†å¤‡ (Preparation)
        feat_ll = self.deep_proj(x_deep) 
        _, skip_lh, skip_hl, skip_hh = self.dwt_idwt.dwt(x_skip)
        
        feat_lh = self.skip_proj(skip_lh)
        feat_hl = self.skip_proj(skip_hl)
        feat_hh = self.skip_proj(skip_hh)
        
        # 2. ç­›é€‰ (Selection)
        #combined = torch.cat([feat_ll, feat_lh, feat_hl, feat_hh], dim=1)
        #combined = self.fusion_conv(combined)

        # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šå…ˆå¯¹é½ï¼Œå†ç­›é€‰ ğŸ”¥ğŸ”¥ğŸ”¥
        # Strip Conv ä¼šåˆ©ç”¨ 7x7 çš„è§†é‡ï¼ŒæŠŠè¯­ä¹‰å’Œçº¹ç†åœ¨ç©ºé—´ä¸Šå¯¹å‡†
        # combined = self.align_module(combined)

        #combined_refined = self.mamba_selector(combined)
        #ref_ll, ref_lh, ref_hl, ref_hh = torch.chunk(combined_refined, 4, dim=1)
        
# ==================== [æ ¸å¿ƒä¿®æ”¹ Start] ====================
        # 2. åˆ†æµå¤„ç† (Divide and Conquer)
        
        # Path A: ä½é¢‘èµ° Mamba (å­¦ä¹ å…¨å±€è¯­ä¹‰)
        ref_ll = self.mamba_ll(feat_ll)
        
        # Path B: é«˜é¢‘èµ° Strip DCN (å­¦ä¹ å‡ ä½•è¾¹ç¼˜)
        # æ‹¼æ¥ä¸‰ä¸ªé«˜é¢‘åˆ†é‡
        high_freq_stack = torch.cat([feat_lh, feat_hl, feat_hh], dim=1)
        
        # Strip DCN å¤„ç†
        ref_high = self.edge_align(high_freq_stack)
        
        # æ‹†åˆ†å›ä¸‰ä¸ªåˆ†é‡
        ref_lh, ref_hl, ref_hh = torch.chunk(ref_high, 3, dim=1)
        # ==================== [æ ¸å¿ƒä¿®æ”¹ End] ======================

        # 3. é‡å»º (Reconstruction)
        out = self.dwt_idwt.idwt(ref_ll, ref_lh, ref_hl, ref_hh)
        return self.out_conv(out)


# ================================================================
# 3. ä¸»æ¨¡å‹: WVM-UNet
# ================================================================
class WVM_UNet(nn.Module):
    # **kwargs ç”¨äºæ¥æ”¶å¹¶å¿½ç•¥ä¸éœ€è¦çš„å‚æ•° (å¦‚ use_dsis ç­‰)
    def __init__(self, n_channels=3, n_classes=1, cnext_type='convnextv2_base', use_decouple=False, **kwargs):
        super().__init__()
        self.use_decouple = use_decouple  # ä¿å­˜å¼€å…³çŠ¶æ€
        use_dcn = kwargs.get('use_dcn', False)
        print(f"ğŸš€ [WVM-UNet] Initializing Model...")
        print(f"   - Alignment Mode: {'Deformable Conv (DCN)' if use_dcn else 'Strip Conv'}")
        print(f"   - MDBES Decoupling: {'ENABLED âœ…' if use_decouple else 'DISABLED âŒ'}") # æ‰“å°çŠ¶æ€
        
        self.n_classes = n_classes

        self.encoder_name = 'cnextv2'
        
        # --- A. Encoder: ConvNeXt V2 ---
        self.enc_model = timm.create_model(
            cnext_type, pretrained=True, features_only=True, 
            out_indices=[0, 1, 2, 3], in_chans=n_channels
        )
        c1, c2, c3, c4 = self.enc_model.feature_info.channels()
        
        # --- B. Decoder: WVM Stages ---
        # Up 1: 1/32 -> 1/16
        self.up1 = WVM_Upsampler(deep_channels=c4, skip_channels=c3, out_channels=c3, use_dcn=use_dcn)
        # Up 2: 1/16 -> 1/8
        self.up2 = WVM_Upsampler(deep_channels=c3, skip_channels=c2, out_channels=c2, use_dcn=use_dcn)
        # Up 3: 1/8 -> 1/4
        self.up3 = WVM_Upsampler(deep_channels=c2, skip_channels=c1, out_channels=c1, use_dcn=use_dcn)
        
        # --- C. Final Head ---
        self.final_up = nn.Sequential(
            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True),
            nn.Conv2d(c1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        # ================= [ä¿®æ”¹ç‚¹ 2: æ ¹æ®å¼€å…³åˆå§‹åŒ–è§£è€¦æ¨¡å—] =================
        if self.use_decouple:
            # 1. åˆå§‹åŒ–è§£è€¦å™¨ (è¾“å…¥ 64 é€šé“)
            self.decoupler = SqueezeBodyEdge(64)
            
            # 2. åˆå§‹åŒ–ä¸¤ä¸ªè¾…åŠ©å¤´ (Version B: å¸¦3x3ç¼“å†²)
            # Body å’Œ Edge éƒ½æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥è¾“å‡ºé€šé“ä¸º 1
            self.head_body = make_head(64, 1)
            self.head_edge = make_head(64, 1)
            
            # 3. æœ€ç»ˆåˆ†å‰²å¤´
            # è¾“å…¥ä¾ç„¶æ˜¯ 64ï¼Œå› ä¸º Body+Edge=FusedFeatureï¼Œé€šé“æ•°ä¸å˜
            self.outc = nn.Conv2d(64, n_classes, kernel_size=1)
        else:
            # åŸå§‹é€»è¾‘ï¼šç›´æ¥æ¥åˆ†ç±»å¤´
            self.outc = nn.Conv2d(64, n_classes, kernel_size=1)


    def forward(self, x):
        features = self.enc_model(x)
        s1, s2, s3, x4 = features[0], features[1], features[2], features[3]

        d1 = self.up1(x_deep=x4, x_skip=s3)
        d2 = self.up2(x_deep=d1, x_skip=s2)
        d3 = self.up3(x_deep=d2, x_skip=s1)
        
        d4 = self.final_up(d3)
        if self.use_decouple:
            # 1. æ˜¾å¼è§£è€¦
            feat_body, feat_edge = self.decoupler(d4)
            
            # 2. é‡è€¦ (Re-couple) -> èåˆä¸€è‡´æ€§ä¸é”åˆ©åº¦
            feat_fuse = feat_body + feat_edge
            
            # 3. ä¸»åˆ†å‰²é¢„æµ‹
            out_seg = self.outc(feat_fuse)
            
            # 4. è®­ç»ƒæ¨¡å¼è¿”å›ä¸‰å…ƒç»„ (ç”¨äºè®¡ç®—é‚£ä¸ªä¸‰åˆä¸€ Loss)
            if self.training:
                out_body = self.head_body(feat_body)
                out_edge = self.head_edge(feat_edge)
                return out_seg, out_body, out_edge
            else:
                # éªŒè¯/æ¨ç†æ¨¡å¼åªè¿”å›æœ€ç»ˆåˆ†å‰²
                return out_seg
        else:
            # åŸå§‹ Baseline é€»è¾‘ (æ— è§£è€¦)
            return self.outc(d4)