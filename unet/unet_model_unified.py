"""
unet_model_unified.py (DSIS Skip-Channel Fix)
ä¿®å¤è¯´æ˜ï¼š
1. ä¿®æ­£äº† up2 å’Œ up3 åˆå§‹åŒ–æ—¶çš„ skip_channels å‚æ•°ï¼Œä½¿å…¶ä½¿ç”¨ç»è¿‡ DSIS åˆ¤æ–­åçš„ skip_c2/skip_c1ï¼Œ
   è€Œä¸æ˜¯åŸå§‹çš„ c2/c1ã€‚è§£å†³äº† 'expected 512 channels, but got 320' çš„æŠ¥é”™ã€‚
2. å®Œæ•´ä¿ç•™äº†åŒæµã€STRGã€CAFM ç­‰é€»è¾‘ã€‚
"""

from .unet_parts import *
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm 

sys.path.insert(0, str(Path(__file__).parent.parent))

# ================================================================
# 1. åŠ¨æ€å¯¼å…¥æ‰€æœ‰å¯é€‰æ¨¡å—
# ================================================================

# PHD è§£ç å™¨æ ¸å¿ƒå—
try: from decoder.hybrid_decoder import PHD_DecoderBlock
except ImportError: PHD_DecoderBlock = None

# åŒæµå¢å¼ºæ¨¡å— (STRG)
try: from .dual_enhance import STRG_Block
except ImportError: STRG_Block = None

# æ˜¾å¼è¾¹ç•Œæµ (Boundary Stream)
try: from .boundary_stream import BoundaryStream
except ImportError: BoundaryStream = None

# WGN (Wavelet Group Norm)
try: from .wgn_module import WGN
except ImportError: WGN = None

# CAFM (Content-Aware Feature Modulation)
try: from .cafm_module import CAFM
except ImportError: CAFM = None

# DSIS (Dual-Stream Interactive Skip)
try: from .dsis_module import DSIS_Module
except ImportError: DSIS_Module = None


class Up_PHD_3Plus(nn.Module):
    """
    ConvNeXt + UNet 3+ + PHD å®Œç¾ç»“åˆç‰ˆ
    æµç¨‹: 
    1. Aggregator: æ”¶é›† [s1,s2,s3,x4] + prev_dec -> ç»Ÿä¸€æ‹¼æ¥ (320ch)
    2. PHD Block: å¯¹ 320ch ç‰¹å¾è¿›è¡Œ Mamba/DCN ç²¾ä¿®
    """
    def __init__(self, current_level, total_levels, enc_ch_list, prev_dec_ch, 
                 out_channels, use_dcn=False, use_dubm=False):
        super().__init__()
        
        # 1. èšåˆå™¨ (UNet 3+ æ ¸å¿ƒ)
        # å‡è®¾ 4å±‚Encoder + 1å±‚Decoderï¼Œæ‹¼æ¥åé€šé“æ•° = 5 * 64 = 320
        cat_channels = 64
        self.aggregator = UNet3P_Aggregator(current_level, total_levels, enc_ch_list, prev_dec_ch, cat_channels)
        
        agg_channels = (len(enc_ch_list) + 1) * cat_channels # 320
        
        # 2. PHD è§£ç å™¨ (å¤„ç†èšåˆåçš„ç‰¹å¾)
        # æ³¨æ„: PHD Block çš„è¾“å…¥æ˜¯ agg_channels (320)ï¼Œè¾“å‡ºæ˜¯ out_channels
        self.phd_block = PHD_DecoderBlock(in_channels=agg_channels, out_channels=out_channels, 
                                          use_dcn=use_dcn, use_dubm=use_dubm)

    def forward(self, prev_dec_feat, enc_feats_list, edge_prior=None):
        # Step 1: å…¨å°ºåº¦èšåˆ
        x_agg = self.aggregator(prev_dec_feat, enc_feats_list)
        
        # Step 2: PHD ç²¾ä¿®
        # PHD Block æœŸæœ›æ¥æ”¶ (x, edge_prior)ã€‚æˆ‘ä»¬å°† x_agg è§†ä¸ºè¾“å…¥ x
        x_out = self.phd_block(x_agg, edge_prior=edge_prior)
        
        return x_out
# ================================================================
# 2. é€‚é…å™¨ï¼šUp_PHD
# ================================================================
class Up_PHD(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True, skip_channels=0, 
                 use_dcn=False, use_dubm=False, use_strg=False):
        super().__init__()
        
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            conv_in_channels = in_channels + skip_channels
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            conv_in_channels = (in_channels // 2) + skip_channels

        # STRG æ¨¡å—
        self.use_strg = use_strg and (STRG_Block is not None)
        if self.use_strg and skip_channels > 0:
            self.strg_enhance = STRG_Block(skip_channels=skip_channels, deep_channels=in_channels)

        self.conv = PHD_DecoderBlock(in_channels=conv_in_channels, out_channels=out_channels, use_dcn=use_dcn, use_dubm=use_dubm)

    def forward(self, x1, x2=None, edge_prior=None):
        x1 = self.up(x1)
        
        if x2 is not None:
            # Padding å¯¹é½
            diffY = x2.size()[2] - x1.size()[2]
            diffX = x2.size()[3] - x1.size()[3]
            if diffX != 0 or diffY != 0:
                x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
            
            if self.use_strg:
                x2 = self.strg_enhance(x_skip=x2, x_deep=x1)
            
            x = torch.cat([x2, x1], dim=1)
        else:
            x = x1
        
        return self.conv(x, edge_prior=edge_prior)


# ================================================================
# 3. ç»Ÿä¸€ä¸»æ¨¡å‹ UNet
# ================================================================
class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=True, 
                 encoder_name='resnet', decoder_name='phd', cnext_type='convnextv2_tiny', 
                 use_wgn_enhancement=False, use_cafm=False, use_edge_loss=False, wgn_orders=None,
                 use_dcn_in_phd=False, use_dsis=False, use_dubm=False, use_strg=False,
                 use_dual_stream=False,
                 use_unet3p=False):
        
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear
        self.encoder_name = encoder_name
        self.decoder_name = decoder_name
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—çš„å¼€å…³
        self.use_dsis = use_dsis and (DSIS_Module is not None)
        self.use_cafm = use_cafm and (CAFM is not None)
        self.use_dual_stream = use_dual_stream and (BoundaryStream is not None)
        self.use_unet3p = use_unet3p  # ğŸ”¥ğŸ”¥ğŸ”¥ åŠ ä¸Šè¿™ä¸€è¡Œï¼
        self.use_dsis = use_dsis and (DSIS_Module is not None)
        # --------------------------------------------------------
        # A. Encoder åˆå§‹åŒ–
        # --------------------------------------------------------
        self.channels = [] 
        if encoder_name == 'resnet':
            from torchvision.models import resnet50, ResNet50_Weights
            resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
            self.enc_stem = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)
            self.layer1 = resnet.layer1
            self.layer2 = resnet.layer2
            self.layer3 = resnet.layer3
            self.layer4 = resnet.layer4
            self.channels = [256, 512, 1024, 2048]
            
        elif encoder_name == 'cnextv2':
            self.enc_model = timm.create_model(cnext_type, pretrained=True, features_only=True, out_indices=[0, 1, 2, 3], in_chans=n_channels)
            self.channels = self.enc_model.feature_info.channels()
            
        else:
            self.inc = DoubleConv(n_channels, 64)
            self.down1 = Down(64, 128)
            self.down2 = Down(128, 256)
            self.down3 = Down(256, 512)
            factor = 2 if bilinear else 1
            self.down4 = Down(512, 1024 // factor)
            self.channels = [128, 256, 512, 1024 // factor]

        c1, c2, c3, c4 = self.channels

        # --------------------------------------------------------
        # B. WGN åˆå§‹åŒ–
        # --------------------------------------------------------
        if use_wgn_enhancement and wgn_orders is not None and WGN is not None:
            print("   âœ¨ Applying WGN Enhancement (Encoder)...")
            def replace_bn_with_wgn(module, order):
                for name, child in module.named_children():
                    if isinstance(child, (nn.BatchNorm2d, nn.GroupNorm)):
                        num_features = child.num_features if isinstance(child, nn.BatchNorm2d) else child.num_channels
                        setattr(module, name, WGN(num_features, order=order))
                    else:
                        replace_bn_with_wgn(child, order)
            if encoder_name == 'resnet':
                replace_bn_with_wgn(self.layer1, wgn_orders['layer1'][0])
                replace_bn_with_wgn(self.layer2, wgn_orders['layer2'][0])
                replace_bn_with_wgn(self.layer3, wgn_orders['layer3'][0])

        # --------------------------------------------------------
        # C. CAFM åˆå§‹åŒ–
        # --------------------------------------------------------
        if self.use_cafm:
            print("   âœ¨ Applying CAFM...")
            self.cafm1 = CAFM(c1)
            self.cafm2 = CAFM(c2)
            self.cafm3 = CAFM(c3)
            self.cafm4 = CAFM(c4)
        
        # --------------------------------------------------------
        # D. DSIS åˆå§‹åŒ– (è®¾ç½® skip_c1 å’Œ skip_c2)
        # --------------------------------------------------------
        if self.use_dsis:
            print("   ğŸ”— Applying DSIS (Dual-Stream Interactive Skip)...")
            dsis_channels = 64 # DSIS è¾“å‡ºå›ºå®šä¸º 64 é€šé“
            self.dsis_module = DSIS_Module(c1_in=c1, c2_in=c2, c_base=dsis_channels)
            
            # ğŸ”¥ è¿™é‡Œçš„è®¡ç®—é€»è¾‘æ˜¯æ­£ç¡®çš„
            skip_c1 = dsis_channels
            skip_c2 = dsis_channels
        else:
            skip_c1 = c1
            skip_c2 = c2

        # --------------------------------------------------------
        # E. åŒæµæ¶æ„ï¼šè¾¹ç•Œæµåˆå§‹åŒ–
        # --------------------------------------------------------
        if self.use_dual_stream:
            print("   ğŸŒŠ [Dual-Stream] Initializing Boundary Stream (Explicit Edge)...")
            self.boundary_stream = BoundaryStream(in_channels=c1)

        # --------------------------------------------------------
        # F. Decoder åˆå§‹åŒ–
        # --------------------------------------------------------
        if self.use_unet3p:
            print("   ğŸŒŸ [Architecture] Enabled UNet 3+ Full-Scale Skip Connections (Perfect Mode)")
            # UNet 3+ Mode
            # Encoder List: [s1, s2, s3, x4] -> å¯¹åº” Channel [c1, c2, c3, c4]
            enc_ch_list = [c1, c2, c3, c4]
            total_levels = 4
            
            # --- Decoder Node 1 (å¯¹åº” s3 åˆ†è¾¨ç‡, Level 2) ---
            # Input: Prev_Decoder(x4/c4), All Encoders
            # Output channels: éšæ„å®šä¹‰ï¼Œé€šå¸¸è¿˜æ˜¯ä¿æŒ c3 æˆ–å‡åŠã€‚PHD å†…éƒ¨ä¼šé™ç»´ã€‚
            # è¿™é‡Œæˆ‘ä»¬è®¾å®šè¾“å‡ºä¸º c3 (384 for tiny)ï¼Œæ–¹ä¾¿åç»­ä¼ é€’
            self.up1 = Up_PHD_3Plus(current_level=2, total_levels=4, enc_ch_list=enc_ch_list, 
                                    prev_dec_ch=c4, out_channels=c3, 
                                    use_dcn=use_dcn_in_phd, use_dubm=use_dubm)
                                    
            # --- Decoder Node 2 (å¯¹åº” s2 åˆ†è¾¨ç‡, Level 1) ---
            # Input: Prev_Decoder(up1 output, c3), All Encoders
            self.up2 = Up_PHD_3Plus(current_level=1, total_levels=4, enc_ch_list=enc_ch_list, 
                                    prev_dec_ch=c3, out_channels=c2, 
                                    use_dcn=use_dcn_in_phd, use_dubm=use_dubm)
                                    
            # --- Decoder Node 3 (å¯¹åº” s1 åˆ†è¾¨ç‡, Level 0) ---
            # Input: Prev_Decoder(up2 output, c2), All Encoders
            self.up3 = Up_PHD_3Plus(current_level=0, total_levels=4, enc_ch_list=enc_ch_list, 
                                    prev_dec_ch=c2, out_channels=c1, 
                                    use_dcn=use_dcn_in_phd, use_dubm=use_dubm)
                                    
            # UNet 3+ æœ€ç»ˆè¾“å‡ºçš„æ˜¯ c1 é€šé“ (s1 å°ºå¯¸)ï¼Œéœ€è¦å†ä¸Šé‡‡æ ·ä¸€æ¬¡å›åŸå›¾
            # åŒæ ·ä½¿ç”¨ DoubleConv æ•´ç†
            if bilinear:
                self.up4 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), DoubleConv(c1, 64))
            else:
                self.up4 = nn.Sequential(nn.ConvTranspose2d(c1, c1 // 2, kernel_size=2, stride=2), DoubleConv(c1 // 2, 64))

        else:
            UpBlock = Up_PHD if decoder_name == 'phd' else Up
        
            if decoder_name == 'phd':
            # Up1 æ¥æ”¶ s3 (c3)ï¼ŒDSIS ä¸å¤„ç† c3ï¼Œæ‰€ä»¥ skip ä»ä¸º c3
                self.up1 = UpBlock(c4, c3, bilinear, skip_channels=c3, use_dcn=use_dcn_in_phd, use_dubm=use_dubm, use_strg=use_strg)
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ [å…³é”®ä¿®å¤]ï¼šè¿™é‡Œå¿…é¡»ç”¨ skip_c2ï¼Œè€Œä¸æ˜¯ c2
                self.up2 = UpBlock(c3, c2, bilinear, skip_channels=skip_c2, use_dcn=use_dcn_in_phd, use_dubm=use_dubm, use_strg=use_strg)
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ [å…³é”®ä¿®å¤]ï¼šè¿™é‡Œå¿…é¡»ç”¨ skip_c1ï¼Œè€Œä¸æ˜¯ c1
                self.up3 = UpBlock(c2, c1, bilinear, skip_channels=skip_c1, use_dcn=use_dcn_in_phd, use_dubm=use_dubm, use_strg=use_strg)
            else:
                self.up1 = UpBlock(c4, c3, bilinear, skip_channels=c3)
            # è¿™é‡Œçš„æ ‡å‡† Decoder æœ€å¥½ä¹Ÿé€‚é…ä¸€ä¸‹ï¼Œè™½ç„¶ä½ ç°åœ¨ä¸»è¦ç”¨ PHD
                self.up2 = UpBlock(c3, c2, bilinear, skip_channels=skip_c2)
                self.up3 = UpBlock(c2, c1, bilinear, skip_channels=skip_c1)

        # æœ€åä¸€å±‚
            if bilinear:
                self.up4 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), DoubleConv(c1, 64))
            else:
                self.up4 = nn.Sequential(nn.ConvTranspose2d(c1, c1 // 2, kernel_size=2, stride=2), DoubleConv(c1 // 2, 64))
        
        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.outc = OutConv(64, n_classes) 

    def forward(self, x):
        # 1. Encoder
        if self.encoder_name == 'cnextv2':
            feats = self.enc_model(x)
            s1, s2, s3, x4 = feats[0], feats[1], feats[2], feats[3]
        elif self.encoder_name == 'resnet':
            x0 = self.enc_stem(x)
            s1 = self.layer1(x0)
            s2 = self.layer2(s1)
            s3 = self.layer3(s2)
            x4 = self.layer4(s3)
        else:
            x1 = self.inc(x)
            s1 = self.down1(x1)
            s2 = self.down2(s1)
            s3 = self.down3(s2)
            x4 = self.down4(s3)

        # 2. CAFM
        if self.use_cafm:
            s1 = self.cafm1(s1)
            s2 = self.cafm2(s2)
            s3 = self.cafm3(s3)
            x4 = self.cafm4(x4)

        # 3. DSIS (æ³¨æ„ï¼šDSIS å’Œ UNet3+ é€šé“é€»è¾‘å¯èƒ½å†²çªï¼ŒUNet3+ æ—¶å»ºè®®å…³é—­ DSIS)
        if self.use_dsis:
            s1, s2 = self.dsis_module(s1, s2)

        # 4. åŒæµ
        boundary_logits = None
        edge_prior = None
        if self.use_dual_stream:
            boundary_logits = self.boundary_stream(s1)
            edge_prior = boundary_logits.detach()

        # 5. Decoder (æ ¸å¿ƒä¿®å¤ç‚¹ï¼šå¢åŠ åˆ†æ”¯åˆ¤æ–­)
        if self.use_unet3p:
            # === ğŸ”¥ UNet 3+ ä¸“ç”¨è·¯å¾„ (å…¨å°ºåº¦èšåˆ) ===
            # å°†æ‰€æœ‰ç‰¹å¾æ‰“åŒ…æˆåˆ—è¡¨: [Scale0(s1), Scale1(s2), Scale2(s3), Scale3(x4)]
            enc_list = [s1, s2, s3, x4]
            
            # Decoder 1: æ¢å¤åˆ° s3 å°ºåº¦
            d1 = self.up1(prev_dec_feat=x4, enc_feats_list=enc_list, edge_prior=edge_prior)
            
            # Decoder 2: æ¢å¤åˆ° s2 å°ºåº¦
            d2 = self.up2(prev_dec_feat=d1, enc_feats_list=enc_list, edge_prior=edge_prior)
            
            # Decoder 3: æ¢å¤åˆ° s1 å°ºåº¦
            d3 = self.up3(prev_dec_feat=d2, enc_feats_list=enc_list, edge_prior=edge_prior)
            
            # Final Up
            d4 = self.up4(d3)
            d5 = self.final_up(d4)
            logits = self.outc(d5)
            
        else:
            # === æ™®é€šè·¯å¾„ (çº§è”è§£ç ) ===
            if self.decoder_name == 'phd':
                # æ³¨æ„ï¼šå¦‚æœ use_dual_stream æ˜¯ Falseï¼Œboundary_logits å°±æ˜¯ None
                d1 = self.up1(x4, s3, edge_prior=boundary_logits)
                d2 = self.up2(d1, s2, edge_prior=boundary_logits)
                d3 = self.up3(d2, s1, edge_prior=boundary_logits)
            else:
                d1 = self.up1(x4, s3)
                d2 = self.up2(d1, s2)
                d3 = self.up3(d2, s1)
                
            d4 = self.up4(d3)
            d5 = self.final_up(d4)
            logits = self.outc(d5)

        # 6. è¿”å›é€»è¾‘
        if self.training and self.use_dual_stream:
            return logits, boundary_logits
        else:
            return logits

      