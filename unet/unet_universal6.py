"""
unet/unet_universal1.py
[Universal Model] å…¨èƒ½å‹ UNet (Final Version)
æ¶æ„ç‰¹ç‚¹:
  1. Spatial Encoder: ConvNeXt V2 (è¯­ä¹‰æå–)
  2. Frequency Encoder: Omni-SFDA Block (é›†æˆäº†æ–¹å‘æ„ŸçŸ¥ã€å¾ªç¯ç¨€ç–ç¼–ç ä¸è½¯é˜ˆå€¼å»å™ªï¼Œä¿®å¤äº†Shortcutç»´åº¦Bug)
  3. Interaction: SK-Fusion (åŸºäºSK-Netæ€æƒ³çš„åŠ¨æ€é€‰æ‹©æ€§èåˆï¼Œè‡ªåŠ¨æŠ—å™ª)
  4. Decoder: Heavy ProtoFormer (3çº§çº§è”äº¤äº’ï¼Œå‚æ•°é‡å¢å¼º) æˆ– Standard (ç¨³å®šå‹ GroupNorm)
  5. Return Logic: ç»Ÿä¸€è¿”å› List æ¥å£ï¼Œé˜²æ­¢è®­ç»ƒå¾ªç¯è§£åŒ…é”™è¯¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

# ================================================================
# 0. åŸºç¡€å·¥å…·ç±» (å°æ³¢å˜æ¢ & Attentionç»„ä»¶)
# ================================================================

class HaarWaveletTransform(nn.Module):
    def __init__(self):
        super().__init__()
        ll = torch.tensor([[0.5, 0.5], [0.5, 0.5]])
        lh = torch.tensor([[-0.5, -0.5], [0.5, 0.5]])
        hl = torch.tensor([[-0.5, 0.5], [-0.5, 0.5]])
        hh = torch.tensor([[0.5, -0.5], [-0.5, 0.5]])
        self.register_buffer('filters', torch.stack([ll, lh, hl, hh]).unsqueeze(1))

    def dwt(self, x):
        B, C, H, W = x.shape
        # Padding é˜²æ­¢å¥‡æ•°å°ºå¯¸æŠ¥é”™
        if H % 2 != 0 or W % 2 != 0: 
            x = F.pad(x, (0, W % 2, 0, H % 2), mode='reflect')
        filters = self.filters.repeat(C, 1, 1, 1)
        output = F.conv2d(x, filters, stride=2, groups=C)
        output = output.view(B, C, 4, output.shape[2], output.shape[3])
        return output[:, :, 0], output[:, :, 1], output[:, :, 2], output[:, :, 3]

class InverseHaarWaveletTransform(nn.Module):
    def __init__(self):
        super().__init__()
        ll = torch.tensor([[1.0, 1.0], [1.0, 1.0]])
        lh = torch.tensor([[-1.0, -1.0], [1.0, 1.0]])
        hl = torch.tensor([[-1.0, 1.0], [-1.0, 1.0]])
        hh = torch.tensor([[1.0, -1.0], [-1.0, 1.0]])
        self.register_buffer('filters', torch.stack([ll, lh, hl, hh]).unsqueeze(1) / 2.0)

    def idwt(self, ll, lh, hl, hh):
        B, C, H, W = ll.shape
        x = torch.cat([ll, lh, hl, hh], dim=1)
        return F.conv_transpose2d(x, self.filters.repeat(C, 1, 1, 1), stride=2, groups=C)

# --- è¾…åŠ©æ¨¡å—: å…¨å±€æ³¨æ„åŠ› (FP32 Safe) ---
class GlobalAttention(nn.Module):
    def __init__(self, dim, num_heads=4, qkv_bias=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # x input: [B, C, H, W] or [B, N, C]
        if x.dim() == 4:
            B, C, H, W = x.shape
            x_in = x.flatten(2).transpose(1, 2) # [B, N, C]
            is_spatial = True
        else:
            B, N, C = x.shape
            x_in = x
            is_spatial = False

        # Pre-Norm
        x_norm = self.norm(x_in)

        # ğŸ”¥ğŸ”¥ğŸ”¥ [FP32 å®‰å…¨åŒº] ğŸ”¥ğŸ”¥ğŸ”¥
        with torch.cuda.amp.autocast(enabled=False):
            x_32 = x_norm.float()
            qkv = self.qkv(x_32).reshape(B, -1, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]

            # é™åˆ¶ Logits æœ€å¤§å€¼ï¼Œé˜²æ­¢ Softmax çˆ†ç‚¸
            attn_logits = (q @ k.transpose(-2, -1)) * self.scale
            attn_logits = torch.clamp(attn_logits, min=-30, max=30) 
            
            attn = attn_logits.softmax(dim=-1)
            x_out = (attn @ v) 
        
        x_out = x_out.to(x.dtype) 
        x_out = x_out.transpose(1, 2).reshape(B, -1, C)
        x_out = self.proj(x_out)
        
        # æ®‹å·®è¿æ¥
        x_out = x_in + x_out

        if is_spatial:
            x_out = x_out.transpose(1, 2).reshape(B, C, H, W)
            
        return x_out

# ================================================================
# 1. æ ¸å¿ƒæ¨¡å—: SFDA Block (é¢‘ç‡æµ - å…¨èƒ½å‹)
# ================================================================

# [ç»„ä»¶] é¢‘ç‡é€šé“æ³¨æ„åŠ›
class FrequencyChannelAttention(nn.Module):
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

# [ç»„ä»¶] å¯å­¦ä¹ è½¯é˜ˆå€¼
class LearnableSoftThresholding(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.threshold = nn.Parameter(torch.tensor([0.02] * channels).view(1, channels, 1, 1))

    def forward(self, x):
        thresh = torch.abs(self.threshold)
        return torch.sign(x) * F.relu(torch.abs(x) - thresh)

# [ç»„ä»¶] æ–¹å‘æ„ŸçŸ¥ç¼–ç å™¨
class DirectionalEncoder(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # LH (æ°´å¹³ä½/å‚ç›´é«˜) -> å‚ç›´çº¹ç†å¼º -> ç”¨ 3x1 å·ç§¯æå–
        self.conv_lh = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 1), padding=(1, 0), bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        # HL (æ°´å¹³é«˜/å‚ç›´ä½) -> æ°´å¹³çº¹ç†å¼º -> ç”¨ 1x3 å·ç§¯æå–
        self.conv_hl = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), padding=(0, 1), bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        # HH (å¯¹è§’çº¿) -> æ— ç‰¹å®šæ–¹å‘ -> ç”¨æ™®é€š 3x3
        self.conv_hh = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        self.fusion = nn.Conv2d(out_channels * 3, out_channels, 1, bias=False)

    def forward(self, lh, hl, hh):
        f_lh = self.conv_lh(lh)
        f_hl = self.conv_hl(hl)
        f_hh = self.conv_hh(hh)
        return self.fusion(torch.cat([f_lh, f_hl, f_hh], dim=1))

# [ç»„ä»¶] å¾ªç¯ç¨€ç–ç¼–ç å—
# [ä¿®æ”¹åçš„ RecurrentSparseBlock] å¢åŠ  GroupNorm å’Œ Clamp ä»¥é˜²æ­¢ NaN
class RecurrentSparseBlock(nn.Module):
    def __init__(self, channels, iterations=2):
        super().__init__()
        self.iterations = iterations
        
        # ä½¿ç”¨ Kaiming åˆå§‹åŒ–é˜²æ­¢åˆå§‹å€¼è¿‡å¤§
        self.encoder = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        self.decoder = nn.Conv2d(channels, channels, 3, padding=1, bias=False)
        
        # âœ… ä¿®æ”¹ 1: GroupNorm -> BatchNorm2d
        self.bn = nn.BatchNorm2d(channels) 
        
        # âœ… ä¿®æ”¹ 2: GroupNorm -> BatchNorm2d
        self.loop_norm = nn.BatchNorm2d(channels)
        
        self.threshold = LearnableSoftThresholding(channels)
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # åˆå§‹ç¼–ç 
        z = self.threshold(self.bn(self.encoder(x)))
        
        for _ in range(self.iterations):
            x_hat = self.decoder(z)
            error = x - x_hat
            delta_z = self.encoder(error)
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ 3: é™åˆ¶æ›´æ–°æ­¥é•¿ï¼Œé˜²æ­¢ä¸€æ­¥è·¨å¤ªå¤§
            delta_z = 0.1 * delta_z
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ 4: æ®‹å·®è¿æ¥åç«‹å³åšå½’ä¸€åŒ–
            z = z + delta_z
            z = self.loop_norm(z) 
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ 5: ç‰©ç†æˆªæ–­ (Hard Clamp)ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡º FP16 èŒƒå›´
            # 20.0 å¯¹äºç‰¹å¾å›¾æ¥è¯´å·²ç»éå¸¸å¤§äº†ï¼Œè¶³å¤Ÿä¿ç•™ä¿¡æ¯ä½†ä¸ä¼šæº¢å‡º
            z = torch.clamp(z, min=-20.0, max=20.0)
            
            z = self.threshold(z)
            
        return z

# [ä¿®æ”¹åçš„ SFDABlock]
class SFDABlock(nn.Module):
    """
    ğŸ”¥ [Omni-SFDA ç¨³å®šç‰ˆ]
    é›†æˆäº†ï¼šFCA, Directional Conv, Recurrent Sparse Coding
    ä¿®å¤äº† Shortcut ç»´åº¦ Bugï¼Œå¹¶å¢å¼ºäº†æ•°å€¼ç¨³å®šæ€§ã€‚
    """
    def __init__(self, in_channels, out_channels, num_heads=4):
        super().__init__()
        self.dwt = HaarWaveletTransform()
        
        # 1. é¢‘ç‡é€‰æ‹©
        self.freq_select = nn.Sequential(
            nn.Conv2d(in_channels * 4, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            FrequencyChannelAttention(out_channels) 
        )

        # 2. ä½é¢‘è·¯å¾„
        self.lo_proj = nn.Conv2d(in_channels, out_channels, 1)
        self.lo_process = nn.Sequential(
            nn.AvgPool2d(kernel_size=2, stride=2), 
            GlobalAttention(out_channels, num_heads=num_heads), 
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        )
        
        # 3. é«˜é¢‘è·¯å¾„
        self.hi_proj_layer = nn.Conv2d(in_channels, out_channels, 1) 
        self.directional_encoder = DirectionalEncoder(out_channels, out_channels)
        # ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„æ–°ç‰ˆ RecurrentSparseBlock
        self.recurrent_denoiser = RecurrentSparseBlock(out_channels, iterations=2)
        
        # 4. èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(out_channels * 3, out_channels, 1),
            nn.BatchNorm2d(out_channels), # ğŸ”¥ ä¿®æ”¹: GN -> BN
            nn.ReLU(inplace=True)
        )
        
        # 5. Shortcut (ä¿æŒä¹‹å‰çš„ä¿®å¤é€»è¾‘)
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.AvgPool2d(kernel_size=2, stride=2), 
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels) # Shortcut å¯ä»¥ä¿ç•™ BN
            )
        else:
            self.shortcut = nn.AvgPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        residual = self.shortcut(x)
        
        # ä¸ºäº†ç¨³å®šæ€§ï¼Œå¯ä»¥åŠ ä¸Š eps é˜²æ­¢é™¤é›¶ (è™½ç„¶ DWT ä¸€èˆ¬æ²¡äº‹)
        x = torch.clamp(x, min=-50, max=50) 
        
        ll, lh, hl, hh = self.dwt.dwt(x)
        
        # Selected View
        all_freq = torch.cat([ll, lh, hl, hh], dim=1)
        feat_selected = self.freq_select(all_freq)
        
        # Low View
        x_lo = self.lo_proj(ll)
        feat_lo = self.lo_process(x_lo)
        
        # High View
        x_lh = self.hi_proj_layer(lh)
        x_hl = self.hi_proj_layer(hl)
        x_hh = self.hi_proj_layer(hh)
        
        feat_hi_dir = self.directional_encoder(x_lh, x_hl, x_hh)
        feat_hi_final = self.recurrent_denoiser(feat_hi_dir)
        
        # Fusion
        # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæœ‰åˆ†æ”¯æ˜¯ NaNï¼Œæ›¿æ¢ä¸º 0 (æç«¯ä¿å‘½æªæ–½ï¼Œå¯é€‰)
        if torch.isnan(feat_hi_final).any():
             feat_hi_final = torch.zeros_like(feat_hi_final)

        out_fused = self.fusion(torch.cat([feat_selected, feat_lo, feat_hi_final], dim=1))
        out_fused = out_fused + residual
        
        return out_fused, out_fused

# ================================================================
# 2. äº¤äº’æ¨¡å— (SK-Fusion)
# ================================================================
class SK_Fusion_Block(nn.Module):
    """
    ğŸ”¥ [SK-Fusion] åŠ¨æ€é€‰æ‹©èåˆæ¨¡å—
    """
    def __init__(self, s_channels, f_channels, reduction=16):
        super().__init__()
        # 1. é¢‘ç‡æµå¯¹é½
        self.f_align = nn.Sequential(
            nn.Conv2d(f_channels, s_channels, 1, bias=False),
            nn.BatchNorm2d(s_channels),
            nn.ReLU(inplace=True)
        )
        
        dim = s_channels
        mid_dim = max(dim // reduction, 32)
        
        # 2. å…¨å±€ä¿¡æ¯æè¿°ç¬¦
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # 3. æƒé‡ç”Ÿæˆå™¨
        self.mlp = nn.Sequential(
            nn.Conv2d(dim, mid_dim, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_dim, dim * 2, 1, bias=False)
        )
        
        self.softmax = nn.Softmax(dim=1)
        
        # 4. æœ€ç»ˆæ•´åˆ
        self.out_conv = nn.Sequential(
            nn.Conv2d(dim, dim, 3, padding=1, bias=False),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x_s, x_f):
        if x_f.shape[2:] != x_s.shape[2:]:
            x_f = F.interpolate(x_f, size=x_s.shape[2:], mode='bilinear', align_corners=True)
        
        x_f_aligned = self.f_align(x_f)
        u = x_s + x_f_aligned
        s = self.avg_pool(u)
        z = self.mlp(s)
        
        b, c, _, _ = x_s.size()
        z = z.view(b, 2, c, 1, 1)
        weights = self.softmax(z)
        
        out = weights[:, 0] * x_s + weights[:, 1] * x_f_aligned
        out = self.out_conv(out)
        
        return out, x_f_aligned

# ================================================================
# 3. è§£ç å™¨ç»„ä»¶: Heavy ProtoFormer
# ================================================================

# ğŸ”¥ [å…³é”®ä¿®æ”¹] æ ‡å‡†è§£ç å™¨åŒå·ç§¯ï¼šå°† BatchNorm æ”¹ä¸º GroupNorm (ç¨³å®šç‰ˆ)
class StandardDoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),  # âœ… æ”¹ä¸º BN
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),  # âœ… æ”¹ä¸º BN
            nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.double_conv(x)

class PrototypeInteractionBlock(nn.Module):
    def __init__(self, channels, num_prototypes=16):
        super().__init__()
        self.channels = channels
        self.prototypes = nn.Parameter(torch.randn(1, num_prototypes, channels))
        self.pos_embed = nn.Parameter(torch.randn(1, channels, 64, 64) * 0.02)
        
        self.q_proj = nn.Conv2d(channels, channels, 1)
        self.k_proj = nn.Linear(channels, channels)
        self.v_proj = nn.Linear(channels, channels)
        self.out_proj = nn.Conv2d(channels, channels, 1)
        self.norm = nn.BatchNorm2d(channels)
        self.local_conv = nn.Sequential(nn.Conv2d(channels, channels, 3, padding=1, groups=channels, bias=False), nn.BatchNorm2d(channels), nn.GELU())
        self.gamma = nn.Parameter(torch.ones(channels) * 1e-5)
    def forward(self, x):
        B, C, H, W = x.shape
        residual = x
        pos = F.interpolate(self.pos_embed, size=(H, W), mode='bilinear', align_corners=False)
        x = x + pos 
        q = self.q_proj(x).flatten(2).transpose(1, 2)
        protos = self.prototypes.repeat(B, 1, 1)
        k = self.k_proj(protos)
        v = self.v_proj(protos)
        
        with torch.cuda.amp.autocast(enabled=False):
            q_32, k_32, v_32 = q.float(), k.float(), v.float()
            scale = C ** -0.5
            attn_logits = (q_32 @ k_32.transpose(-2, -1)) * scale
            attn_logits = torch.clamp(attn_logits, min=-30, max=30)
            attn = attn_logits.softmax(dim=-1)
            out = attn @ v_32
            
        out = out.to(x.dtype)
        out = out.transpose(1, 2).view(B, C, H, W)
        out = self.out_proj(out)
        out = out + self.local_conv(out)
        return self.norm(residual + out * self.gamma.view(1, -1, 1, 1))

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(dim, hidden_dim, 1),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Conv2d(hidden_dim, dim, 1),
            nn.Dropout(dropout)
        )
    def forward(self, x): return self.net(x)

class PHD_DecoderBlock_Pro(nn.Module):
    def __init__(self, in_channels, out_channels, depth=3): 
        super().__init__()
        self.align = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)
        )
        self.layers = nn.ModuleList([])
        self.gamma_ffn = nn.Parameter(torch.ones(depth, out_channels) * 1e-5)
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PrototypeInteractionBlock(out_channels, num_prototypes=16),
                FeedForward(out_channels, out_channels * 4)
            ]))

    def forward(self, x):
        x = self.align(x)
        for i, (proto_block, ffn) in enumerate(self.layers):
            x = proto_block(x)
            gamma = self.gamma_ffn[i].view(1, -1, 1, 1)
            x = x + gamma * ffn(x)
        return x

class Up_Universal(nn.Module):
    def __init__(self, in_channels, out_channels, skip_channels=0, decoder_type='phd'):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        conv_in = in_channels + skip_channels
        if decoder_type == 'phd':
            self.conv = PHD_DecoderBlock_Pro(conv_in, out_channels, depth=2)
        else:
            self.conv = StandardDoubleConv(conv_in, out_channels)

    def forward(self, x1, x2=None):
        x1 = self.up(x1)
        if x2 is not None:
            diffY, diffX = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]
            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
            x = torch.cat([x2, x1], dim=1)
        else:
            x = x1
        return self.conv(x)

# ================================================================
# 4. ä¸»æ¨¡å‹: UniversalUNet
# ================================================================
class UniversalUNet(nn.Module):
    def __init__(self, 
                 n_classes=1, 
                 cnext_type='convnextv2_tiny', 
                 pretrained=True,
                 decoder_type='phd',       
                 use_dual_stream=True,     
                 use_deep_supervision=False,
                 **kwargs):
        super().__init__()
        self.n_classes = n_classes
        self.use_dual_stream = use_dual_stream
        self.decoder_type = decoder_type
        self.use_deep_supervision = use_deep_supervision
        
        print(f"ğŸ¤– [Universal Model] Initialized with:")
        print(f"   - Encoder: {cnext_type}")
        print(f"   - Dual Stream (SFDA): {'âœ… ON (Omni-Optimized)' if use_dual_stream else 'âŒ OFF'}")
        print(f"   - Interaction: SK-Fusion")
        print(f"   - Decoder: {decoder_type.upper()}")
        
        # 1. Spatial Encoder
        self.spatial_encoder = timm.create_model(cnext_type, pretrained=pretrained, features_only=True, out_indices=(0, 1, 2, 3), drop_path_rate=0.0)
        s_dims = self.spatial_encoder.feature_info.channels() 
        
        # 2. Frequency Encoder
        if self.use_dual_stream:
            f_dims = [c // 2 for c in s_dims]
            self.freq_stem = nn.Sequential(nn.Conv2d(3, f_dims[0], 4, stride=4, padding=0), nn.BatchNorm2d(f_dims[0]), nn.ReLU(True))
            
            self.freq_layers = nn.ModuleList([
                SFDABlock(in_channels=f_dims[i], out_channels=f_dims[i+1]) 
                for i in range(3)
            ])
            
            self.bi_fgf_modules = nn.ModuleList([SK_Fusion_Block(s_dims[i], f_dims[i]) for i in range(4)])
            self.edge_head = nn.Sequential(nn.Conv2d(f_dims[0], 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 1, 1))

        # 3. Decoder
        self.up1 = Up_Universal(s_dims[3], s_dims[2], skip_channels=s_dims[2], decoder_type=decoder_type)
        self.up2 = Up_Universal(s_dims[2], s_dims[1], skip_channels=s_dims[1], decoder_type=decoder_type)
        self.up3 = Up_Universal(s_dims[1], s_dims[0], skip_channels=s_dims[0], decoder_type=decoder_type)
        
        self.final_up = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)
        self.outc = nn.Conv2d(s_dims[0], n_classes, kernel_size=1)
        
        # 4. Deep Supervision
        if self.use_deep_supervision:
            self.head_up2 = nn.Sequential(nn.Conv2d(s_dims[1], 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.Conv2d(32, n_classes, 1))
            self.head_up3 = nn.Sequential(nn.Conv2d(s_dims[0], 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.Conv2d(32, n_classes, 1))

    def forward(self, x):
        s_feats = list(self.spatial_encoder(x))
        
        # Dual Stream
        edge_logits = None
        if self.use_dual_stream:
            f_curr = self.freq_stem(x)
            f_feats = [f_curr]
            for layer in self.freq_layers:
                f_next, f_inter = layer(f_curr)
                f_feats.append(f_inter)
                f_curr = f_next
            
            s_fused_list = []
            for i in range(4):
                s_out, _ = self.bi_fgf_modules[i](s_feats[i], f_feats[i])
                s_fused_list.append(s_out)
            s_feats = s_fused_list
            
            if self.training:
                edge_small = self.edge_head(f_feats[0])
                edge_logits = F.interpolate(edge_small, size=x.shape[2:], mode='bilinear', align_corners=True)

        # Decoder
        d1 = self.up1(s_feats[3], s_feats[2])
        d2 = self.up2(d1, s_feats[1])
        d3 = self.up3(d2, s_feats[0])
        
        logits = self.outc(self.final_up(d3))
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ [ç»Ÿä¸€è¿”å› List] ğŸ”¥ğŸ”¥ğŸ”¥
        if self.training:
            outputs = [logits]
            
            if self.use_deep_supervision:
                aux2 = self.head_up2(d2)
                aux3 = self.head_up3(d3)
                outputs.extend([aux2, aux3])
            
            if self.use_dual_stream and edge_logits is not None:
                outputs.append(edge_logits)
                
            return outputs # å§‹ç»ˆè¿”å› Listï¼Œtrain02.py èˆ’æœäº†

        return logits