"""
unet/unet_universal1.py
[Universal Model] å…¨èƒ½å‹ UNet (Final Version)
æ¶æ„ç‰¹ç‚¹:
  1. Spatial Encoder: ConvNeXt V2 (è¯­ä¹‰æå–)
  2. Frequency Encoder: SFDA Block (é¢‘è°±-é¢‘ç‡è§£è€¦æ³¨æ„åŠ›, Hi-Lo Attention)
     - åŒ…å« FP32 ç²¾åº¦ä¿æŠ¤ (é˜² NaN)
     - åŒ…å« LayerNorm + Residual (é˜²æ¢¯åº¦æ¶ˆå¤±)
  3. Interaction: Bi-FGF (åŒå‘é—¨æ§èåˆ)
  4. Decoder: ProtoFormer (åŸå‹äº¤äº’è§£ç å™¨, FP32 ä¿æŠ¤)
  5. Deep Supervision: æ”¯æŒå¤šå°ºåº¦è¾…åŠ©ç›‘ç£
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

# ================================================================
# 0. åŸºç¡€å·¥å…·ç±» (å°æ³¢å˜æ¢ & Attentionç»„ä»¶)
# ================================================================

class HaarWaveletTransform(nn.Module):
    def __init__(self):
        super().__init__()
        ll = torch.tensor([[0.5, 0.5], [0.5, 0.5]])
        lh = torch.tensor([[-0.5, -0.5], [0.5, 0.5]])
        hl = torch.tensor([[-0.5, 0.5], [-0.5, 0.5]])
        hh = torch.tensor([[0.5, -0.5], [-0.5, 0.5]])
        self.register_buffer('filters', torch.stack([ll, lh, hl, hh]).unsqueeze(1))

    def dwt(self, x):
        B, C, H, W = x.shape
        # Padding é˜²æ­¢å¥‡æ•°å°ºå¯¸æŠ¥é”™
        if H % 2 != 0 or W % 2 != 0: 
            x = F.pad(x, (0, W % 2, 0, H % 2), mode='reflect')
        filters = self.filters.repeat(C, 1, 1, 1)
        output = F.conv2d(x, filters, stride=2, groups=C)
        output = output.view(B, C, 4, output.shape[2], output.shape[3])
        return output[:, :, 0], output[:, :, 1], output[:, :, 2], output[:, :, 3]

class InverseHaarWaveletTransform(nn.Module):
    def __init__(self):
        super().__init__()
        ll = torch.tensor([[1.0, 1.0], [1.0, 1.0]])
        lh = torch.tensor([[-1.0, -1.0], [1.0, 1.0]])
        hl = torch.tensor([[-1.0, 1.0], [-1.0, 1.0]])
        hh = torch.tensor([[1.0, -1.0], [-1.0, 1.0]])
        self.register_buffer('filters', torch.stack([ll, lh, hl, hh]).unsqueeze(1) / 2.0)

    def idwt(self, ll, lh, hl, hh):
        B, C, H, W = ll.shape
        x = torch.cat([ll, lh, hl, hh], dim=1)
        return F.conv_transpose2d(x, self.filters.repeat(C, 1, 1, 1), stride=2, groups=C)

# --- è¾…åŠ©æ¨¡å—: å…¨å±€æ³¨æ„åŠ› (FP32 Safe + Residual + Norm) ---
class GlobalAttention(nn.Module):
    def __init__(self, dim, num_heads=4, qkv_bias=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)
        
        # ğŸ”¥ [å…³é”®ä¼˜åŒ–] LayerNormï¼Œä¿è¯æ·±å±‚è®­ç»ƒç¨³å®š
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # x input: [B, C, H, W] or [B, N, C]
        if x.dim() == 4:
            B, C, H, W = x.shape
            x_in = x.flatten(2).transpose(1, 2) # [B, N, C]
            is_spatial = True
        else:
            B, N, C = x.shape
            x_in = x
            is_spatial = False

        # Pre-Norm
        x_norm = self.norm(x_in)

        # ğŸ”¥ğŸ”¥ğŸ”¥ [FP32 å®‰å…¨åŒº] é˜²æ­¢ Attention æº¢å‡ºå¯¼è‡´ NaN ğŸ”¥ğŸ”¥ğŸ”¥
        with torch.cuda.amp.autocast(enabled=False):
            x_32 = x_norm.float()
            qkv = self.qkv(x_32).reshape(B, -1, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]

            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            x_out = (attn @ v)
        
        x_out = x_out.to(x.dtype) # è½¬å› FP16/FP32

         # ğŸ”¥ğŸ”¥ğŸ”¥ [æ ¸å¿ƒä¿®å¤] åˆå¹¶å¤šå¤´ç»´åº¦ ğŸ”¥ğŸ”¥ğŸ”¥
        # [B, heads, N, dim_head] -> [B, N, heads, dim_head] -> [B, N, C]
        x_out = x_out.transpose(1, 2).reshape(B, -1, C)
        x_out = self.proj(x_out)
        
        # ğŸ”¥ [å…³é”®ä¼˜åŒ–] åŠ ä¸Šæ®‹å·®è¿æ¥ Input + Output
        x_out = x_in + x_out

        if is_spatial:
            x_out = x_out.transpose(1, 2).reshape(B, C, H, W)
            
        return x_out

# --- è¾…åŠ©æ¨¡å—: çª—å£å±€éƒ¨æ³¨æ„åŠ› (FP32 Safe + Residual) ---
class WindowAttention(nn.Module):
    def __init__(self, dim, num_heads=4, window_size=7):
        super().__init__()
        self.window_size = window_size
        # å¤ç”¨ GlobalAttention (å†…éƒ¨å·²æœ‰ Norm å’Œ Residual)
        self.attn = GlobalAttention(dim, num_heads) 

    def forward(self, x):
        B, C, H, W = x.shape
        # Pad å¦‚æœå°ºå¯¸ä¸èƒ½è¢« window_size æ•´é™¤
        pad_h = (self.window_size - H % self.window_size) % self.window_size
        pad_w = (self.window_size - W % self.window_size) % self.window_size
        x_padded = F.pad(x, (0, pad_w, 0, pad_h))
        
        _, _, Hp, Wp = x_padded.shape
        
        # Window Partition
        # [B, C, Hp, Wp] -> [B*NumWin, C, WinSize, WinSize]
        x_windows = F.unfold(x_padded, kernel_size=self.window_size, stride=self.window_size)
        x_windows = x_windows.transpose(1, 2).contiguous().view(B, -1, C, self.window_size, self.window_size)
        x_windows = x_windows.permute(0, 1, 3, 4, 2).contiguous().view(-1, C, self.window_size, self.window_size)
        
        # Attention (å†…éƒ¨æœ‰ Residual)
        # è¿™é‡Œçš„ Residual æ˜¯é’ˆå¯¹ window å†…éƒ¨ç‰¹å¾çš„
        attn_windows = self.attn(x_windows)
        
        # Window Reverse
        attn_windows = attn_windows.view(B, -1, C, self.window_size, self.window_size).permute(0, 2, 3, 4, 1)
        attn_windows = attn_windows.contiguous().view(B, C * self.window_size * self.window_size, -1)
        x_out = F.fold(attn_windows, output_size=(Hp, Wp), kernel_size=self.window_size, stride=self.window_size)
        
        # Crop Padding
        return x_out[:, :, :H, :W]
# ================================================================
# æ–°å¢æ¨¡å—: ASPP (Atrous Spatial Pyramid Pooling)
# ä½œç”¨: å¢åŠ æ„Ÿå—é‡ï¼Œæ˜¾è‘—å¢åŠ æœ‰æ•ˆå‚æ•°é‡ (é’ˆå¯¹ç­–ç•¥2)
# ================================================================
class ASPP(nn.Module):
    def __init__(self, in_channels, out_channels, atrous_rates=[6, 12, 18]):
        super(ASPP, self).__init__()
        modules = []
        
        # 1. åˆ†æ”¯1: 1x1 å·ç§¯
        modules.append(nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)))

        # 2. åˆ†æ”¯2-4: ä¸åŒæ‰©å¼ ç‡çš„ 3x3 ç©ºæ´å·ç§¯
        for rate in atrous_rates:
            modules.append(nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)))

        self.aspp_blocks = nn.ModuleList(modules)
        
        # 3. åˆ†æ”¯5: å…¨å±€å¹³å‡æ± åŒ– (Image Pooling)
        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True))

        # 4. èåˆæŠ•å½±å±‚
        # è¾“å…¥é€šé“æ•° = 5 ä¸ªåˆ†æ”¯ * out_channels
        self.project = nn.Sequential(
            nn.Conv2d(out_channels * (len(atrous_rates) + 2), out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1) # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )

    def forward(self, x):
        res = []
        # è®¡ç®—å·ç§¯åˆ†æ”¯
        for block in self.aspp_blocks:
            res.append(block(x))
        
        # è®¡ç®—æ± åŒ–åˆ†æ”¯å¹¶ä¸Šé‡‡æ ·
        res.append(F.interpolate(self.global_avg_pool(x), size=x.shape[2:], mode='bilinear', align_corners=True))
        
        # æ‹¼æ¥
        res = torch.cat(res, dim=1)
        
        # èåˆè¾“å‡º
        return self.project(res)
# ================================================================
# 1. æ ¸å¿ƒæ¨¡å—: SFDA Block (æ›¿ä»£ WaveletMambaBlock)
# ================================================================

class SFDABlock(nn.Module):
    """
    [New Core] Spectral-Frequency Decoupled Attention Block
    é¢‘ç‡æµæ ¸å¿ƒï¼šä½é¢‘å…¨å±€ + é«˜é¢‘å±€éƒ¨ + é—¨æ§èåˆ + æ®‹å·®ä¿®æ­£
    """
    def __init__(self, in_channels, out_channels, num_heads=4):
        super().__init__()
        self.dwt = HaarWaveletTransform()
        
        # 1. ä½é¢‘è·¯å¾„ (Lo-Path): å¤„ç† LL
        self.lo_proj = nn.Conv2d(in_channels, out_channels, 1)
        self.lo_process = nn.Sequential(
            nn.AvgPool2d(kernel_size=2, stride=2), # ä¸‹é‡‡æ ·
            GlobalAttention(out_channels, num_heads=num_heads), # å†…éƒ¨æœ‰Res+Norm
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        )
        
        # 2. é«˜é¢‘è·¯å¾„ (Hi-Path): å¤„ç† LH, HL, HH
        self.hi_proj = nn.Conv2d(in_channels * 3, out_channels, 1)
        self.hi_process = WindowAttention(out_channels, num_heads=num_heads, window_size=7)
        
        # 3. ä¼˜åŒ–åçš„é—¨æ§èåˆ
        self.gate = nn.Sequential(
            nn.Conv2d(out_channels * 2, 1, 1),
            nn.Sigmoid()
        )
        
        # 4. ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # 5. ğŸ”¥ [æ ¸å¿ƒä¿®å¤ 1] æ®‹å·®è·¯å¾„å¿…é¡»ä¸‹é‡‡æ ·ï¼
        # å› ä¸º DWT ä¼šè®©ä¸»è·¯å°ºå¯¸å‡åŠï¼Œæ‰€ä»¥æ®‹å·®è·¯ä¹Ÿè¦å‡åŠæ‰èƒ½ç›¸åŠ 
        self.shortcut = nn.Sequential(
            nn.AvgPool2d(kernel_size=2, stride=2), # ç©ºé—´ä¸‹é‡‡æ ·
            nn.Conv2d(in_channels, out_channels, 1), # é€šé“å¯¹é½
            nn.BatchNorm2d(out_channels)
        )

        # 6. ğŸ”¥ [æ ¸å¿ƒä¿®å¤ 2] ç§»é™¤ self.downsample
        # SFDA Block æœ¬èº«é€šè¿‡ DWT å·²ç»å®Œæˆäº†ä¸‹é‡‡æ · (Stride 2)ï¼Œ
        # ä¸éœ€è¦å†åœ¨æœ«å°¾åŠ  downsampleï¼Œå¦åˆ™ä¸€ä¸ª Block é™é‡‡æ · 4 å€ä¼šå¯¼è‡´å’Œ ConvNeXt å¯¹ä¸ä¸Šã€‚

    def forward(self, x):
        # 0. å‡†å¤‡æ®‹å·® (ç°åœ¨ residual ä¹Ÿæ˜¯ H/2, W/2 äº†)
        residual = self.shortcut(x)

        # 1. DWT åˆ†è§£ (H/2, W/2)
        ll, lh, hl, hh = self.dwt.dwt(x)
        
        # 2. Lo-Path
        x_lo = self.lo_proj(ll)
        out_lo = self.lo_process(x_lo)
        
        # 3. Hi-Path
        x_hi = torch.cat([lh, hl, hh], dim=1)
        x_hi = self.hi_proj(x_hi)
        out_hi = self.hi_process(x_hi)
        
        # 4. Gated Fusion
        gate_input = torch.cat([out_lo, out_hi], dim=1)
        gate_map = self.gate(gate_input)
        
        out_fused = self.fusion(torch.cat([out_lo, out_hi * gate_map], dim=1))
        
        # 5. æ®‹å·®ç›¸åŠ  (ç°åœ¨å°ºå¯¸åŒ¹é…äº†ï¼)
        out_fused = out_fused + residual
        
        # ğŸ”¥ [æ ¸å¿ƒä¿®å¤ 3] ç›´æ¥è¿”å› out_fused
        # out_fused å·²ç»æ˜¯ä¸‹ä¸€å±‚éœ€è¦çš„å°ºå¯¸ (Stride 2)
        # next_layer_input = out_fused
        # interaction_feat = out_fused
        return out_fused, out_fused

# ================================================================
# 2. äº¤äº’æ¨¡å— (Bi-FGF)
# ================================================================
class Cross_GL_FGF(nn.Module):
    def __init__(self, s_channels, f_channels, reduction=16):
        super().__init__()
        s_mid, f_mid = max(s_channels // reduction, 4), max(f_channels // reduction, 4)
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.mlp_s2f = nn.Sequential(nn.Linear(s_channels, f_mid, bias=False), nn.ReLU(), nn.Linear(f_mid, f_channels, bias=False), nn.Sigmoid())
        self.mlp_f2s = nn.Sequential(nn.Linear(f_channels, s_mid, bias=False), nn.ReLU(), nn.Linear(s_mid, s_channels, bias=False), nn.Sigmoid())
        self.spatial_s2f = nn.Sequential(nn.Conv2d(s_channels, 1, 7, padding=3, bias=False), nn.BatchNorm2d(1), nn.Sigmoid())
        self.spatial_f2s = nn.Sequential(nn.Conv2d(f_channels, 1, 7, padding=3, bias=False), nn.BatchNorm2d(1), nn.Sigmoid())
        self.f_align = nn.Conv2d(f_channels, s_channels, 1)
        self.fusion_conv = nn.Sequential(nn.Conv2d(s_channels * 2, s_channels, 1), nn.BatchNorm2d(s_channels), nn.ReLU(inplace=True))

    def forward(self, x_s, x_f):
        B, Cs, H, W = x_s.shape
        _, Cf, _, _ = x_f.shape
        if x_f.shape[2:] != x_s.shape[2:]: x_f = F.interpolate(x_f, size=(H, W), mode='bilinear')
        s_vec, f_vec = self.gap(x_s).view(B, Cs), self.gap(x_f).view(B, Cf)
        f_clean = x_f * self.mlp_s2f(s_vec).view(B, Cf, 1, 1)
        s_clean = x_s * self.mlp_f2s(f_vec).view(B, Cs, 1, 1)
        f_refined = f_clean * self.spatial_s2f(s_clean) + f_clean
        s_refined = s_clean * self.spatial_f2s(f_clean) + s_clean
        out = self.fusion_conv(torch.cat([s_refined, self.f_align(f_refined)], dim=1))
        return out, f_refined

# ================================================================
# 3. è§£ç å™¨ç»„ä»¶: ProtoFormer & Standard
# ================================================================
class StandardDoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.double_conv(x)

class PrototypeInteractionBlock(nn.Module):
    """
    [ProtoFormer Core] åŸå‹äº¤äº’å•å…ƒ (FP32 Safe)
    """
    def __init__(self, channels, num_prototypes=16):
        super().__init__()
        self.channels = channels
        self.prototypes = nn.Parameter(torch.randn(1, num_prototypes, channels))
        self.pos_embed = nn.Parameter(torch.randn(1, channels, 64, 64) * 0.02)
        
        self.q_proj = nn.Conv2d(channels, channels, 1)
        self.k_proj = nn.Linear(channels, channels)
        self.v_proj = nn.Linear(channels, channels)
        self.out_proj = nn.Conv2d(channels, channels, 1)
        self.norm = nn.GroupNorm(8, channels)
        self.local_conv = nn.Sequential(nn.Conv2d(channels, channels, 3, padding=1, groups=channels, bias=False), nn.BatchNorm2d(channels), nn.GELU())

    def forward(self, x):
        B, C, H, W = x.shape
        residual = x
        pos = F.interpolate(self.pos_embed, size=(H, W), mode='bilinear', align_corners=False)
        x = x + pos 
        q = self.q_proj(x).flatten(2).transpose(1, 2)
        protos = self.prototypes.repeat(B, 1, 1)
        k = self.k_proj(protos)
        v = self.v_proj(protos)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ [FP32 å®‰å…¨åŒº] é˜²æ­¢ Decoder NaN ğŸ”¥ğŸ”¥ğŸ”¥
        with torch.cuda.amp.autocast(enabled=False):
            q_32, k_32, v_32 = q.float(), k.float(), v.float()
            scale = C ** -0.5
            attn = (q_32 @ k_32.transpose(-2, -1)) * scale
            attn = attn.softmax(dim=-1)
            out = attn @ v_32
            
        out = out.to(x.dtype)
        out = out.transpose(1, 2).view(B, C, H, W)
        out = self.out_proj(out)
        out = out + self.local_conv(out)
        return self.norm(out + residual)

class PHD_DecoderBlock_Pro(nn.Module):
    def __init__(self, in_channels, out_channels): 
        super().__init__()
        self.align = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)
        )
        # æ¯ä¸ªè§£ç å±‚ç‹¬ç«‹çš„åŸå‹
        self.proto_block = PrototypeInteractionBlock(out_channels, num_prototypes=16)
        
    def forward(self, x):
        return self.proto_block(self.align(x))

class Up_Universal(nn.Module):
    def __init__(self, in_channels, out_channels, skip_channels=0, decoder_type='phd'):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        conv_in = in_channels + skip_channels
        if decoder_type == 'phd':
            self.conv = PHD_DecoderBlock_Pro(conv_in, out_channels)
        else:
            self.conv = StandardDoubleConv(conv_in, out_channels)

    def forward(self, x1, x2=None):
        x1 = self.up(x1)
        if x2 is not None:
            diffY, diffX = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]
            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
            x = torch.cat([x2, x1], dim=1)
        else:
            x = x1
        return self.conv(x)

# ================================================================
# 4. ä¸»æ¨¡å‹: UniversalUNet (æœ€ç»ˆç»„è£…)
# ================================================================
class UniversalUNet(nn.Module):
    def __init__(self, 
                 n_classes=1, 
                 cnext_type='convnextv2_tiny', 
                 pretrained=True,
                 decoder_type='phd',       
                 use_dual_stream=True,     
                 use_deep_supervision=False,
                 **kwargs):
        super().__init__()
        self.n_classes = n_classes
        self.use_dual_stream = use_dual_stream
        self.decoder_type = decoder_type
        self.use_deep_supervision = use_deep_supervision
        
        print(f"ğŸ¤– [Universal Model] Initialized with:")
        print(f"   - Encoder: {cnext_type} (Pretrained={pretrained})")
        print(f"   - Dual Stream (SFDA + HiLo): {'âœ… ON' if use_dual_stream else 'âŒ OFF'}")
        print(f"   - Decoder: {decoder_type}")
        print(f"   - Deep Supervision: {'âœ… ON' if use_deep_supervision else 'âŒ OFF'}")

        # 1. Spatial Encoder
        self.spatial_encoder = timm.create_model(cnext_type, pretrained=pretrained, features_only=True, out_indices=(0, 1, 2, 3), drop_path_rate=0.0)
        s_dims = self.spatial_encoder.feature_info.channels() # [96, 192, 384, 768] for tiny
        
        # 2. Frequency Encoder (SFDA Stream)
        if self.use_dual_stream:
            f_dims = [c // 4 for c in s_dims]
            self.freq_stem = nn.Sequential(nn.Conv2d(3, f_dims[0], 4, stride=4, padding=0), nn.BatchNorm2d(f_dims[0]), nn.ReLU(True))
            
            # ğŸ”¥ ä½¿ç”¨ä¿®å¤åçš„ SFDABlock (å¸¦ Shortcut å’Œ Gateä¼˜åŒ–)
            self.freq_layers = nn.ModuleList([
                SFDABlock(in_channels=f_dims[i], out_channels=f_dims[i+1]) 
                for i in range(3)
            ])
            
            self.bi_fgf_modules = nn.ModuleList([Cross_GL_FGF(s_dims[i], f_dims[i]) for i in range(4)])
            self.edge_head = nn.Sequential(nn.Conv2d(f_dims[0], 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 1, 1))
# ğŸ”¥ğŸ”¥ğŸ”¥ [æ–°å¢ä¿®æ”¹] ç­–ç•¥2: å®šä¹‰è¯­ä¹‰æ¡¥æ¢ ASPP ğŸ”¥ğŸ”¥ğŸ”¥
        # æ”¾åœ¨ Encoder æœ€æ·±å±‚ (s_dims[3]=768) ä¹‹å
        # è¾“å…¥è¾“å‡ºä¿æŒä¸€è‡´ï¼Œåªä¸ºäº†æå–ç‰¹å¾å’Œå¢åŠ å‚æ•°
        self.bridge = ASPP(in_channels=s_dims[3], out_channels=s_dims[3])
        # 3. Decoder
        self.up1 = Up_Universal(s_dims[3], s_dims[2], skip_channels=s_dims[2], decoder_type=decoder_type)
        self.up2 = Up_Universal(s_dims[2], s_dims[1], skip_channels=s_dims[1], decoder_type=decoder_type)
        self.up3 = Up_Universal(s_dims[1], s_dims[0], skip_channels=s_dims[0], decoder_type=decoder_type)
        
        self.final_up = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)
        self.outc = nn.Conv2d(s_dims[0], n_classes, kernel_size=1)
        
        # 4. Deep Supervision Heads
        if self.use_deep_supervision:
            # Scale 1/8
            self.head_up2 = nn.Sequential(
                nn.Conv2d(s_dims[1], 32, 3, padding=1), 
                nn.BatchNorm2d(32), nn.ReLU(), 
                nn.Conv2d(32, n_classes, 1)
            )
            # Scale 1/4
            self.head_up3 = nn.Sequential(
                nn.Conv2d(s_dims[0], 32, 3, padding=1), 
                nn.BatchNorm2d(32), nn.ReLU(), 
                nn.Conv2d(32, n_classes, 1)
            )

    def forward(self, x):
        # 1. Encoder Pass
        s_feats = list(self.spatial_encoder(x))
        
        # 2. Dual Stream Pass (SFDA)
        edge_logits = None
        if self.use_dual_stream:
            f_curr = self.freq_stem(x)
            f_feats = [f_curr]
            for layer in self.freq_layers:
                f_next, f_inter = layer(f_curr) # nextæ˜¯ä¸‹ä¸€å±‚è¾“å…¥ï¼Œinteræ˜¯å½“å‰å±‚ç”¨äºäº¤äº’çš„ç‰¹å¾
                f_feats.append(f_inter)
                f_curr = f_next
            
            # Interaction
            s_fused_list = []
            f_enhanced_list = []
            for i in range(4):
                s_out, f_out = self.bi_fgf_modules[i](s_feats[i], f_feats[i])
                s_fused_list.append(s_out)
                f_enhanced_list.append(f_out)
            s_feats = s_fused_list
            
            if self.training:
                edge_small = self.edge_head(f_enhanced_list[0])
                edge_logits = F.interpolate(edge_small, size=x.shape[2:], mode='bilinear', align_corners=True)
# ğŸ”¥ğŸ”¥ğŸ”¥ [æ–°å¢ä¿®æ”¹] ç­–ç•¥2: åœ¨è¿›å…¥è§£ç å™¨ä¹‹å‰ï¼Œå…ˆè¿‡æ¡¥ ğŸ”¥ğŸ”¥ğŸ”¥
        # s_feats[3] æ˜¯æœ€æ·±å±‚è¯­ä¹‰ç‰¹å¾ (H/32)
        # é€šè¿‡ ASPP å¢å¼ºå…¶å…¨å±€æ„Ÿå—é‡
        s_feats[3] = self.bridge(s_feats[3])
        # 3. Decoder Pass
        s1, s2, s3, s4 = s_feats
        
        d1 = self.up1(s4, s3)
        d2 = self.up2(d1, s2)
        d3 = self.up3(d2, s1)
        
        logits = self.outc(self.final_up(d3))
        
        # 4. Deep Supervision Return Logic
        if self.training and self.use_deep_supervision:
            aux2 = self.head_up2(d2)
            aux3 = self.head_up3(d3)
            outputs = [logits, aux2, aux3]
            if self.use_dual_stream and edge_logits is not None:
                outputs.append(edge_logits)
            return outputs
        
        # Legacy Return Logic
        if self.training and self.use_dual_stream and edge_logits is not None:
            return logits, edge_logits
            
        return logits