""" Full assembly of the parts to form the complete network """

from .unet_parts import *
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ advanced_cafm_module
sys.path.insert(0, str(Path(__file__).parent.parent))
from advanced_cafm_module import Advanced_CAFM


class EdgeDecoder(nn.Module):
    """
    è¾¹ç¼˜è§£ç å™¨åˆ†æ”¯ï¼šé€‚é… WGN V3 çš„ 3å€é€šé“è¾“å‡º
    çº§è”ä¸Šé‡‡æ ·ï¼Œæœ€ç»ˆè¾“å‡ºåŸå›¾å°ºå¯¸è¾¹ç¼˜å›¾
    """

    def __init__(self):
        super().__init__()

        # --- Layer 3 (High Freq) ---
        # WGN V3 è¾“å‡º: 1024 * 3 = 3072 é€šé“
        # è¾“å…¥ 16x16 -> ä¸Šé‡‡æ · -> 32x32
        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv1 = nn.Sequential(
            nn.Conv2d(3072, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )

        # --- Layer 2 (High Freq) ---
        # WGN V3 è¾“å‡º: 512 * 3 = 1536 é€šé“
        # è¾“å…¥ 32x32 + 32x32 -> æ‹¼æ¥ -> 64x64
        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv2 = nn.Sequential(
            nn.Conv2d(512 + 1536, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        # --- Layer 1 (High Freq) ---
        # WGN V3 è¾“å‡º: 256 * 3 = 768 é€šé“
        # è¾“å…¥ 64x64 + 64x64 -> æ‹¼æ¥ -> 128x128
        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv3 = nn.Sequential(
            nn.Conv2d(256 + 768, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # æœ€ç»ˆè¾“å‡ºå±‚
        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x5_h, x4_h, x3_h):
        # x5_h: [B, 3072, 16, 16]
        x = self.conv1(self.up1(x5_h))  # -> [512, 32, 32]

        # x4_h: [B, 1536, 32, 32]
        x = torch.cat([x, x4_h], dim=1)  # -> [2048, 32, 32]
        x = self.conv2(self.up2(x))  # -> [256, 64, 64]

        # x3_h: [B, 768, 64, 64]
        x = torch.cat([x, x3_h], dim=1)  # -> [1024, 64, 64]
        x = self.conv3(self.up3(x))  # -> [64, 128, 128]

        # æœ€ç»ˆä¸Šé‡‡æ ·å›åŸå›¾ (128 -> 256)
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)  # -> [64, 256, 256]
        return self.final_conv(x)


class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=False, use_advanced_cafm=False, use_resnet_encoder=False, use_wgn_enhancement=False, wgn_orders=None):
        super(UNet, self).__init__()
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear
        self.use_advanced_cafm = use_advanced_cafm
        self.use_resnet_encoder = use_resnet_encoder
        self.use_wgn_enhancement = use_wgn_enhancement
        self.checkpointing = False  # é»˜è®¤ä¸å¯ç”¨ gradient checkpointing
        
        if use_resnet_encoder:
            # ========== ResNet50ç¼–ç å™¨ ==========
            from torchvision.models import resnet50, ResNet50_Weights
            
            # è®¾ç½®é»˜è®¤çš„WGN orderé…ç½®
            if wgn_orders is None:
                wgn_orders = {
                    'layer1': (3, 2),  # 256é€šé“ï¼Œè¾ƒå°çš„order
                    'layer2': (4, 3),  # 512é€šé“ï¼Œä¸­ç­‰order  
                    'layer3': (5, 4)   # 1024é€šé“ï¼Œè¾ƒå¤§çš„order
                }
            
            resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
            
            # æå–ResNet50å„å±‚
            self.conv1 = resnet.conv1      # Out: 64ch, Stride=2 (Size 128x128)
            self.bn1 = resnet.bn1
            self.relu = resnet.relu
            self.maxpool = resnet.maxpool   # Out: 64ch, Stride=2 (Size 64x64)
            
            self.layer1 = resnet.layer1     # Out: 256ch, Stride=1 (Size 64x64)
            self.layer2 = resnet.layer2     # Out: 512ch, åŸStride=2
            self.layer3 = resnet.layer3     # Out: 1024ch, åŸStride=2
            
            # ğŸ”¥ã€å…³é”®ä¿®æ”¹ 1ã€‘: å¼ºåˆ¶ä¿®æ”¹ Layer2 å’Œ Layer3 çš„ Stride ä¸º 1
            # è¿™æ ·å®ƒä»¬å°±ä¸ä¼šåœ¨å†…éƒ¨è¿›è¡Œä¸‹é‡‡æ ·äº†
            self.layer2[0].conv2.stride = (1, 1)
            self.layer2[0].downsample[0].stride = (1, 1)
            
            self.layer3[0].conv2.stride = (1, 1)
            self.layer3[0].downsample[0].stride = (1, 1)

            # ğŸ”¥ã€å…³é”®ä¿®æ”¹ 2ã€‘: å®šä¹‰æ˜¾å¼çš„ä¸‹é‡‡æ ·å±‚
            # é¡ºåº: WGNå¢å¼º -> è·³è·ƒè¿æ¥å¼•å‡º -> ä¸‹é‡‡æ · -> ä¸‹ä¸€å±‚
            self.explicit_down_to_layer2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.explicit_down_to_layer3 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.explicit_down_to_bottleneck = nn.MaxPool2d(kernel_size=2, stride=2)
            
            # ========== WGNå¢å¼ºæ¨¡å—ï¼ˆå¯é€‰ï¼‰==========
            if use_wgn_enhancement:
                # ç›´æ¥ä» wgn åŒ…å¯¼å…¥ (é»˜è®¤æŒ‡å‘ __init__.py é‡Œå®šä¹‰çš„é‚£ä¸ª)
                from wgn import Wg_nConv_Block
                self.wgn_enhance1 = Wg_nConv_Block(256, *wgn_orders['layer1'])  # layer1åå¢å¼º
                self.wgn_enhance2 = Wg_nConv_Block(512, *wgn_orders['layer2'])  # layer2åå¢å¼º  
                self.wgn_enhance3 = Wg_nConv_Block(1024, *wgn_orders['layer3']) # layer3åå¢å¼º
                # åˆå§‹åŒ–è¾¹ç¼˜è§£ç å™¨
                self.edge_decoder = EdgeDecoder()

            # ========== ç“¶é¢ˆå±‚å¤„ç†æ¨¡å— (CAFM / Conv) ==========
            # è¾“å…¥æ¥è‡ª Layer3(1024) -> Down -> Bottleneck(1024)
            if use_advanced_cafm:
                self.cafm = Advanced_CAFM(n_feat=1024, n_head=8)
            self.bottleneck_conv = DoubleConv(1024, 1024)
            
            # ========== è§£ç å™¨ï¼ˆå¯¹ç§°ç»“æ„ï¼‰==========
            # æŒ‰ç…§å›¾ç¤ºé€»è¾‘é…ç½®é€šé“æ•°
            
            # Up1: æ¥æ”¶ Bottleneck(1024), æ‹¼æ¥ WGN3/Layer3(1024) -> è¾“å‡º 512
            self.up1 = Up(
                in_channels=1024,
                out_channels=512,
                bilinear=bilinear,
                skip_channels=1024
            )
            
            # Up2: æ¥æ”¶ Up1(512), æ‹¼æ¥ WGN2/Layer2(512) -> è¾“å‡º 256
            self.up2 = Up(
                in_channels=512,
                out_channels=256,
                bilinear=bilinear,
                skip_channels=512
            )
            
            # Up3: æ¥æ”¶ Up2(256), æ‹¼æ¥ WGN1/Layer1(256) -> è¾“å‡º 64
            # æ³¨æ„: ä¸‹ä¸€æ­¥è¦æ‹¼ Conv1(64)ï¼Œæ‰€ä»¥è¿™é‡Œè¾“å‡º 64
            self.up3 = Up(
                in_channels=256,
                out_channels=64,
                bilinear=bilinear,
                skip_channels=256
            )
            
            # Up4: æ¥æ”¶ Up3(64), æ‹¼æ¥ Conv1(64) -> è¾“å‡º 64
            self.up4 = Up(
                in_channels=64,
                out_channels=64,
                bilinear=bilinear,
                skip_channels=64
            )
            
            # ========== è¾“å‡ºå±‚ ==========
            self.outc = OutConv(64, n_classes)
            
        else:
            # ========== åŸå§‹U-Netç¼–ç å™¨ (ä¿ç•™åŸå§‹é€»è¾‘) ==========
            # ç¼–ç å™¨ï¼ˆä¸‹é‡‡æ ·ï¼‰éƒ¨åˆ†
            self.inc = (DoubleConv(n_channels, 64))
            self.down1 = (Down(64, 128))
            self.down2 = (Down(128, 256))
            self.down3 = (Down(256, 512))
            factor = 2 if bilinear else 1
            self.down4 = (Down(512, 1024 // factor))
            
            # æ¡ä»¶æ€§åœ°åˆ›å»º Advanced_CAFM æ¨¡å—
            if self.use_advanced_cafm:
                bottleneck_channels = 1024 // factor
                self.advanced_cafm_bottleneck = Advanced_CAFM(n_feat=bottleneck_channels)
            
            # è§£ç å™¨ï¼ˆä¸Šé‡‡æ ·ï¼‰éƒ¨åˆ†
            self.up1 = (Up(1024, 512 // factor, bilinear))
            self.up2 = (Up(512, 256 // factor, bilinear))
            self.up3 = (Up(256, 128 // factor, bilinear))
            self.up4 = (Up(128, 64, bilinear))
            self.outc = (OutConv(64, n_classes))

    def forward(self, x):
        if self.use_resnet_encoder:
            # ========== ResNet50ç¼–ç å™¨å‰å‘ä¼ æ’­ (é‡æ„ç‰ˆ) ==========
            # å‡è®¾è¾“å…¥: [B, 3, 256, 256]
            
            # 1. Stem (Conv1)
            x1 = self.relu(self.bn1(self.conv1(x))) # [B, 64, 128, 128]
            x1_skip = x1 # ä¿å­˜ç”¨äº Up4
            
            x2 = self.maxpool(x1) # [B, 64, 64, 64] -> ä¸‹é‡‡æ ·
            
            # 2. Layer 1
            x3 = self.layer1(x2) # [B, 256, 64, 64]
            x3_high = None
            if self.use_wgn_enhancement:
                x3, x3_high = self.wgn_enhance1(x3) # WGNå¢å¼º
            
            x3_skip = x3 # ä¿å­˜ç”¨äº Up3 (æœªä¸‹é‡‡æ ·)
            x3_down = self.explicit_down_to_layer2(x3) # [B, 256, 32, 32] -> ä¸‹é‡‡æ ·
            
            # 3. Layer 2 (Strideå·²æ”¹1)
            x4 = self.layer2(x3_down) # [B, 512, 32, 32]
            x4_high = None
            if self.use_wgn_enhancement:
                x4, x4_high = self.wgn_enhance2(x4)
                
            x4_skip = x4 # ä¿å­˜ç”¨äº Up2 (æœªä¸‹é‡‡æ ·)
            x4_down = self.explicit_down_to_layer3(x4) # [B, 512, 16, 16] -> ä¸‹é‡‡æ ·
            
            # 4. Layer 3 (Strideå·²æ”¹1)
            x5 = self.layer3(x4_down) # [B, 1024, 16, 16]
            x5_high = None
            if self.use_wgn_enhancement:
                x5, x5_high = self.wgn_enhance3(x5)
            
            x5_skip = x5 # ä¿å­˜ç”¨äº Up1 (æœªä¸‹é‡‡æ ·)
            x5_down = self.explicit_down_to_bottleneck(x5) # [B, 1024, 8, 8] -> ä¸‹é‡‡æ ·
            
            # ========== ç“¶é¢ˆå±‚å¤„ç† ==========
            if self.use_advanced_cafm:
                x_bot = self.cafm(x5_down) # [B, 1024, 8, 8]
            else:
                x_bot = self.bottleneck_conv(x5_down)
            
            # ========== è§£ç å™¨ (ä¸¥æ ¼å¯¹åº”è·³è·ƒè¿æ¥) ==========
            # Up1: 8->16, Concat x5_skip (Layer3 WGN out)
            x = self.up1(x_bot, x5_skip) # -> [B, 512, 16, 16]
            
            # Up2: 16->32, Concat x4_skip (Layer2 WGN out)
            x = self.up2(x, x4_skip) # -> [B, 256, 32, 32]
            
            # Up3: 32->64, Concat x3_skip (Layer1 WGN out)
            x = self.up3(x, x3_skip) # -> [B, 64, 64, 64]
            
            # Up4: 64->128, Concat x1_skip (Conv1 out)
            x = self.up4(x, x1_skip) # -> [B, 64, 128, 128]
            
            # ========== è¾“å‡º ==========
            logits = self.outc(x) # [B, n_classes, 128, 128]
            
            # æœ€åä¸Šé‡‡æ ·å›åŸå›¾ (128 -> 256)
            logits = F.interpolate(logits, scale_factor=2, mode='bilinear', align_corners=True)
            
            # è®­ç»ƒæ—¶è¿”å›è¾¹ç¼˜è§£ç å™¨ç»“æœ
            if self.use_wgn_enhancement and self.training:
                logits_edge = self.edge_decoder(x5_high, x4_high, x3_high)
                return logits, logits_edge
            else:
                return logits
        
        else:
            # ========== åŸå§‹U-Netå‰å‘ä¼ æ’­ ==========
            if self.checkpointing:
                x1 = torch.utils.checkpoint.checkpoint(self.inc, x, use_reentrant=False)
                x2 = torch.utils.checkpoint.checkpoint(self.down1, x1, use_reentrant=False)
                x3 = torch.utils.checkpoint.checkpoint(self.down2, x2, use_reentrant=False)
                x4 = torch.utils.checkpoint.checkpoint(self.down3, x3, use_reentrant=False)
                x5 = torch.utils.checkpoint.checkpoint(self.down4, x4, use_reentrant=False)
                
                if self.use_advanced_cafm:
                    x5 = torch.utils.checkpoint.checkpoint(self.advanced_cafm_bottleneck, x5, use_reentrant=False)
                
                x = torch.utils.checkpoint.checkpoint(self.up1, x5, x4, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up2, x, x3, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up3, x, x2, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up4, x, x1, use_reentrant=False)
                logits = torch.utils.checkpoint.checkpoint(self.outc, x, use_reentrant=False)
            else:
                x1 = self.inc(x)
                x2 = self.down1(x1)
                x3 = self.down2(x2)
                x4 = self.down3(x3)
                x5 = self.down4(x4)
                
                if self.use_advanced_cafm:
                    x5 = self.advanced_cafm_bottleneck(x5)
                
                x = self.up1(x5, x4)
                x = self.up2(x, x3)
                x = self.up3(x, x2)
                x = self.up4(x, x1)
                logits = self.outc(x)
            return logits

    def use_checkpointing(self):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        self.checkpointing = True
        
    def disable_checkpointing(self):
        """ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        self.checkpointing = False