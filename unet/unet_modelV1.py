""" Full assembly of the parts to form the complete network """

from .unet_parts import *
# ä»åŒåŒ…ä¸‹çš„ unet_parts.py å¯¼å…¥æ‰€æœ‰æ„ä»¶ï¼ˆDoubleConv / Down / Up / OutConvï¼‰
import sys
from pathlib import Path
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ advanced_cafm_module
sys.path.insert(0, str(Path(__file__).parent.parent))
from advanced_cafm_module import Advanced_CAFM


# åœ¨ unet_model.py çš„ class UNet(nn.Module): ä¹‹å‰æ’å…¥ä»¥ä¸‹ç±»

class EdgeDecoder(nn.Module):
    """
    è¾¹ç¼˜è§£ç å™¨åˆ†æ”¯ï¼šé€‚é… WGN V3 çš„ 3å€é€šé“è¾“å‡º
    """

    def __init__(self):
        super().__init__()

        # --- Layer 3 (High Freq) ---
        # WGN V3 è¾“å‡º: 1024 * 3 = 3072 é€šé“
        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv1 = nn.Sequential(
            # ä¿®æ”¹ç‚¹ 1: è¾“å…¥é€šé“ä» 1024 æ”¹ä¸º 3072 (1024*3)
            nn.Conv2d(3072, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True)
        )

        # --- Layer 2 (High Freq) ---
        # WGN V3 è¾“å‡º: 512 * 3 = 1536 é€šé“
        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.conv2 = nn.Sequential(
            # ä¿®æ”¹ç‚¹ 2: æ‹¼æ¥åé€šé“æ•° = ä¸Šå±‚ä¸‹æ¥çš„(512) + æœ¬å±‚WGNçš„(1536) = 2048
            nn.Conv2d(512 + 1536, 256, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        # --- Layer 1 (High Freq) ---
        # WGN V3 è¾“å‡º: 256 * 3 = 768 é€šé“
        self.up3 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)  # ç›´æ¥ä¸Šé‡‡æ ·4å€
        self.conv3 = nn.Sequential(
            # ä¿®æ”¹ç‚¹ 3: æ‹¼æ¥åé€šé“æ•° = ä¸Šå±‚ä¸‹æ¥çš„(256) + æœ¬å±‚WGNçš„(768) = 1024
            nn.Conv2d(256 + 768, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # æœ€ç»ˆè¾“å‡º
        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x5_h, x4_h, x3_h):
        # x5_h: [B, 3072, 16, 16]
        x = self.conv1(self.up1(x5_h))  # -> [512, 32, 32]

        # x4_h: [B, 1536, 32, 32]
        x = torch.cat([x, x4_h], dim=1)  # -> [2048, 32, 32]
        x = self.conv2(self.up2(x))  # -> [256, 64, 64]

        # x3_h: [B, 768, 64, 64]
        x = torch.cat([x, x3_h], dim=1)  # -> [1024, 64, 64]
        x = self.conv3(x)  # -> [64, 64, 64]

        # æœ€ç»ˆä¸Šé‡‡æ ·å›åŸå›¾
        x = self.up3(x)  # -> [64, 256, 256]
        return self.final_conv(x)
# ---------------------------------------------------------
class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, bilinear=False, use_advanced_cafm=False, use_resnet_encoder=False, use_wgn_enhancement=False, wgn_orders=None):
        super(UNet, self).__init__()
        # n_channels: è¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼ˆRGB=3ï¼‰
        # n_classes: è¾“å‡ºç±»åˆ«æ•°ï¼ˆè¯­ä¹‰åˆ†å‰²çš„ç±»åˆ«æ•°é‡ï¼›äºŒåˆ†ç±»=2 æˆ– 1 è§†å®ç°è€Œå®šï¼‰
        # bilinear: ä¸Šé‡‡æ ·æ–¹å¼ï¼ŒTrue=åŒçº¿æ€§æ’å€¼ï¼ŒFalse=åå·ç§¯ï¼ˆConvTranspose2dï¼‰
        # use_advanced_cafm: æ˜¯å¦åœ¨ç“¶é¢ˆå±‚ä½¿ç”¨ Advanced_CAFM æ¨¡å—è¿›è¡Œç‰¹å¾å¢å¼º
        # use_resnet_encoder: æ˜¯å¦ä½¿ç”¨ResNet50ä½œä¸ºç¼–ç å™¨
        # use_wgn_enhancement: æ˜¯å¦åœ¨ResNet50ç¼–ç å™¨å„å±‚åæ·»åŠ WGNå¢å¼º
        # wgn_orders: WGNå—çš„orderé…ç½®ï¼Œæ ¼å¼ä¸º{'layer1': (order_low, order_high), ...}
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear
        self.use_advanced_cafm = use_advanced_cafm
        self.use_resnet_encoder = use_resnet_encoder
        self.use_wgn_enhancement = use_wgn_enhancement
        self.checkpointing = False  # é»˜è®¤ä¸å¯ç”¨ gradient checkpointing
        
        if use_resnet_encoder:
            # ========== ResNet50ç¼–ç å™¨ ==========
            from torchvision.models import resnet50, ResNet50_Weights
            import torch.nn.functional as F
            
            # è®¾ç½®é»˜è®¤çš„WGN orderé…ç½®
            if wgn_orders is None:
                wgn_orders = {
                    'layer1': (3, 2),  # 256é€šé“ï¼Œè¾ƒå°çš„order
                    'layer2': (4, 3),  # 512é€šé“ï¼Œä¸­ç­‰order  
                    'layer3': (5, 4)   # 1024é€šé“ï¼Œè¾ƒå¤§çš„order
                }
            
            resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
            
            # æå–ResNet50å„å±‚ï¼ˆèˆå¼ƒlayer4ï¼‰
            self.conv1 = resnet.conv1      # è¾“å‡º: 64é€šé“, stride=2
            self.bn1 = resnet.bn1
            self.relu = resnet.relu
            self.maxpool = resnet.maxpool   # stride=2
            self.layer1 = resnet.layer1     # è¾“å‡º: 256é€šé“, stride=1
            self.layer2 = resnet.layer2     # è¾“å‡º: 512é€šé“, stride=2
            self.layer3 = resnet.layer3     # è¾“å‡º: 1024é€šé“, stride=2
            # layer4ä¸ä½¿ç”¨
            
            # ========== WGNå¢å¼ºæ¨¡å—ï¼ˆå¯é€‰ï¼‰==========
            if use_wgn_enhancement:
                # ç›´æ¥ä» wgn åŒ…å¯¼å…¥ (é»˜è®¤æŒ‡å‘ __init__.py é‡Œå®šä¹‰çš„é‚£ä¸ª)
                from wgn import Wg_nConv_Block
                self.wgn_enhance1 = Wg_nConv_Block(256, *wgn_orders['layer1'])  # layer1åå¢å¼º
                self.wgn_enhance2 = Wg_nConv_Block(512, *wgn_orders['layer2'])  # layer2åå¢å¼º  
                self.wgn_enhance3 = Wg_nConv_Block(1024, *wgn_orders['layer3']) # layer3åå¢å¼º
                # ğŸ”¥ã€æ–°å¢ã€‘åˆå§‹åŒ–è¾¹ç¼˜è§£ç å™¨
                self.edge_decoder = EdgeDecoder()
            # ========== è¾“å…¥ç‰¹å¾åˆ†æ”¯ï¼ˆç”¨äºæœ€åçš„è·³è·ƒè¿æ¥ï¼‰==========
            self.input_branch = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )
            
            # ========== ç“¶é¢ˆå±‚å¤„ç†æ¨¡å—ï¼ˆå¼€å…³æ§åˆ¶ï¼‰==========
            # use_advanced_cafm=True: ä½¿ç”¨CAFMæ³¨æ„åŠ›å¢å¼º
            if use_advanced_cafm:
                self.cafm = Advanced_CAFM(n_feat=1024, n_head=8)
            # use_advanced_cafm=False: ä½¿ç”¨ä¼ ç»Ÿå·ç§¯å—ï¼ˆåŸºçº¿å¯¹æ¯”ï¼‰
            self.bottleneck_conv = DoubleConv(1024, 1024)
            
            # ========== è§£ç å™¨ï¼ˆæ ‡å‡†UNetå¯¹ç§°ç»“æ„ï¼Œè½¬ç½®å·ç§¯ç‰ˆæœ¬ï¼‰==========
            # up1: 16Ã—16 â†’ 32Ã—32ï¼Œæ‹¼æ¥layer2è¾“å‡º(512é€šé“)
            self.up1 = Up(
                in_channels=1024,    # ç“¶é¢ˆå±‚è¾“å‡º1024é€šé“
                out_channels=512,
                bilinear=bilinear,
                skip_channels=512    # layer2(x4)çš„é€šé“æ•°
            )
            
            # up2: 32Ã—32 â†’ 64Ã—64ï¼Œæ‹¼æ¥layer1è¾“å‡º(256é€šé“)
            self.up2 = Up(
                in_channels=512,
                out_channels=256,
                bilinear=bilinear,
                skip_channels=256    # layer1(x3)çš„é€šé“æ•°
            )
            
            # up3: 64Ã—64 â†’ 128Ã—128ï¼Œæ‹¼æ¥conv1è¾“å‡º(64é€šé“) â­æ–°è®¾è®¡ï¼
            self.up3 = Up(
                in_channels=256,
                out_channels=64,
                bilinear=bilinear,
                skip_channels=64     # conv1(x1)çš„é€šé“æ•°
            )
            
            # up4: 128Ã—128 â†’ 256Ã—256ï¼Œæ‹¼æ¥input_branchè¾“å‡º(64é€šé“)
            self.up4 = Up(
                in_channels=64,
                out_channels=64,
                bilinear=bilinear,
                skip_channels=64     # input_branchçš„é€šé“æ•°
            )
            
            # ========== è¾“å‡ºå±‚ ==========
            self.outc = OutConv(64, n_classes)
            
        else:
            # ========== åŸå§‹U-Netç¼–ç å™¨ ==========
            # ç¼–ç å™¨ï¼ˆä¸‹é‡‡æ ·ï¼‰éƒ¨åˆ†ï¼šæ¯èµ°ä¸€å±‚ï¼Œç‰¹å¾é€šé“æ•°å¢åŠ ï¼Œç©ºé—´åˆ†è¾¨ç‡å‡åŠ
            self.inc = (DoubleConv(n_channels, 64))# ç¬¬ä¸€å±‚ï¼šè¾“å…¥é€šé“ -> 64ï¼Œåšä¸¤æ¬¡(Conv-BN-ReLU)
            self.down1 = (Down(64, 128))# ä¸‹é‡‡æ ·åˆ° 1/2ï¼Œé€šé“ 64->128
            self.down2 = (Down(128, 256)) # ä¸‹é‡‡æ ·åˆ° 1/4ï¼Œé€šé“ 128->256
            self.down3 = (Down(256, 512)) # ä¸‹é‡‡æ ·åˆ° 1/8ï¼Œé€šé“ 256->512
            factor = 2 if bilinear else 1  # è‹¥ç”¨åŒçº¿æ€§ä¸Šé‡‡æ ·ï¼Œä¸ºäº†ä¿æŒå‚æ•°é‡ï¼Œé€šé“å‡åŠ
            self.down4 = (Down(512, 1024 // factor))  # ä¸‹é‡‡æ ·åˆ° 1/16ï¼Œé€šé“ 512->(1024//factor)
            
            # æ¡ä»¶æ€§åœ°åˆ›å»º Advanced_CAFM æ¨¡å—ç”¨äºç“¶é¢ˆå±‚å¢å¼º
            if self.use_advanced_cafm:
                bottleneck_channels = 1024 // factor
                self.advanced_cafm_bottleneck = Advanced_CAFM(n_feat=bottleneck_channels)
            
            # è§£ç å™¨ï¼ˆä¸Šé‡‡æ ·ï¼‰éƒ¨åˆ†ï¼šæ¯èµ°ä¸€å±‚ï¼Œä¸Šé‡‡æ · + æ‹¼æ¥è·³è¿ï¼ˆskip-connectionï¼‰+ åŒå·ç§¯
            self.up1 = (Up(1024, 512 // factor, bilinear))# ç”±æœ€åº•éƒ¨å‘ä¸Šï¼Œé€šé“åˆå¹¶åå†é™åˆ° 512//factor
            self.up2 = (Up(512, 256 // factor, bilinear))# å†å‘ä¸Šï¼š512 -> 256//factor
            self.up3 = (Up(256, 128 // factor, bilinear))# å†å‘ä¸Šï¼š256 -> 128//factor
            self.up4 = (Up(128, 64, bilinear))# å†å‘ä¸Šï¼š128 -> 64
            self.outc = (OutConv(64, n_classes)) # æœ€åä¸€å±‚ 1x1 å·ç§¯ï¼ŒæŠŠé€šé“æ•°å˜æˆç±»åˆ«æ•°

    def forward(self, x):
        if self.use_resnet_encoder:
            # ========== ResNet50ç¼–ç å™¨å‰å‘ä¼ æ’­ ==========
            import torch.nn.functional as F
            
            # è¾“å…¥ç‰¹å¾æå–ï¼ˆç”¨äºæœ€åçš„è·³è·ƒè¿æ¥ï¼‰
            input_features = self.input_branch(x)  # [B, 64, H, W] (256Ã—256)
            
            # ResNet50ç¼–ç å™¨
            x1 = self.relu(self.bn1(self.conv1(x)))  # [B, 64, H/2, W/2] (128Ã—128)
            x2 = self.maxpool(x1)                     # [B, 64, H/4, W/4] (64Ã—64)
            x3 = self.layer1(x2)                      # [B, 256, H/4, W/4] (64Ã—64)
            x3_high = None  # å ä½ç¬¦

            # WGNå¢å¼ºlayer1è¾“å‡ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_wgn_enhancement:
                x3, x3_high= self.wgn_enhance1(x3)           # [B, 256, H/4, W/4] (64Ã—64) WGNå¢å¼º
            
            x4 = self.layer2(x3)                      # [B, 512, H/8, W/8] (32Ã—32)
            
            # WGNå¢å¼ºlayer2è¾“å‡ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_wgn_enhancement:
                x4, x4_high = self.wgn_enhance2(x4)           # [B, 512, H/8, W/8] (32Ã—32) WGNå¢å¼º
            
            x5 = self.layer3(x4)                      # [B, 1024, H/16, W/16] (16Ã—16)
            x5_high = None

            # WGNå¢å¼ºlayer3è¾“å‡ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_wgn_enhancement:
                x5, x5_high = self.wgn_enhance3(x5)           # [B, 1024, H/16, W/16] (16Ã—16) WGNå¢å¼º
            
            # ========== ç“¶é¢ˆå±‚å¤„ç†ï¼ˆå¼€å…³æ§åˆ¶ï¼‰==========
            if self.use_advanced_cafm:
                # ä½¿ç”¨CAFMæ³¨æ„åŠ›å¢å¼º
                x5 = self.cafm(x5)  # [B, 1024, H/16, W/16] (16Ã—16)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå·ç§¯å—ï¼ˆåŸºçº¿å¯¹æ¯”ï¼‰
                x5 = self.bottleneck_conv(x5)  # [B, 1024, H/16, W/16] (16Ã—16)
            
            # ========== è§£ç å™¨ï¼ˆæ ‡å‡†UNetå¯¹ç§°ç»“æ„ï¼‰==========
            # up1: ä¸Šé‡‡æ ·åˆ°32Ã—32ï¼Œæ‹¼æ¥layer2è¾“å‡º(x4)
            x = self.up1(x5, x4)  # [B, 512, H/8, W/8] (32Ã—32)
            
            # up2: ä¸Šé‡‡æ ·åˆ°64Ã—64ï¼Œæ‹¼æ¥layer1è¾“å‡º(x3)
            x = self.up2(x, x3)  # [B, 256, H/4, W/4] (64Ã—64)
            
            # up3: ä¸Šé‡‡æ ·åˆ°128Ã—128ï¼Œæ‹¼æ¥conv1è¾“å‡º(x1) â­æ–°è®¾è®¡ï¼
            x = self.up3(x, x1)  # [B, 64, H/2, W/2] (128Ã—128)
            
            # up4: ä¸Šé‡‡æ ·åˆ°256Ã—256ï¼Œæ‹¼æ¥input_branchè¾“å‡º
            x = self.up4(x, input_features)  # [B, 64, H, W] (256Ã—256)
            
            # ========== è¾“å‡º ==========
            logits = self.outc(x)  # [B, n_classes, H, W] (256Ã—256)
            # ğŸ”¥ã€ä¿®æ”¹ã€‘å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ä¸”å¼€äº†WGNï¼Œè¿”å›åŒç»“æœ
            if self.use_wgn_enhancement and self.training:
                logits_edge = self.edge_decoder(x5_high, x4_high, x3_high)
                return logits, logits_edge
            else:
                return logits
        
        else:
            # ========== åŸå§‹U-Netå‰å‘ä¼ æ’­ ==========
            # ç¼–ç è·¯å¾„ï¼šä¸€è·¯ä¸‹é‡‡æ ·å¹¶ä¿å­˜ä¸­é—´ç»“æœç”¨äºè·³è¿
            if self.checkpointing:
                # ä½¿ç”¨ gradient checkpointing èŠ‚çœæ˜¾å­˜
                x1 = torch.utils.checkpoint.checkpoint(self.inc, x, use_reentrant=False)
                x2 = torch.utils.checkpoint.checkpoint(self.down1, x1, use_reentrant=False)
                x3 = torch.utils.checkpoint.checkpoint(self.down2, x2, use_reentrant=False)
                x4 = torch.utils.checkpoint.checkpoint(self.down3, x3, use_reentrant=False)
                x5 = torch.utils.checkpoint.checkpoint(self.down4, x4, use_reentrant=False)
                
                # å¦‚æœå¯ç”¨äº† Advanced_CAFMï¼Œå¯¹ç“¶é¢ˆå±‚ç‰¹å¾è¿›è¡Œå¢å¼º
                if self.use_advanced_cafm:
                    x5 = torch.utils.checkpoint.checkpoint(self.advanced_cafm_bottleneck, x5, use_reentrant=False)
                
                # è§£ç è·¯å¾„ï¼šä¸Šé‡‡æ · + ä¸å¯¹åº”ç¼–ç å±‚çš„ç‰¹å¾å›¾æ‹¼æ¥ï¼ˆU å½¢ç»“æ„çš„"è·³è¿"ï¼‰
                x = torch.utils.checkpoint.checkpoint(self.up1, x5, x4, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up2, x, x3, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up3, x, x2, use_reentrant=False)
                x = torch.utils.checkpoint.checkpoint(self.up4, x, x1, use_reentrant=False)
                logits = torch.utils.checkpoint.checkpoint(self.outc, x, use_reentrant=False)
            else:
                # æ­£å¸¸å‰å‘ä¼ æ’­
                x1 = self.inc(x)# å°ºå¯¸ä¸å˜ï¼Œé€šé“ 64
                x2 = self.down1(x1)# ç©ºé—´ 1/2ï¼Œé€šé“ 128
                x3 = self.down2(x2) # ç©ºé—´ 1/4ï¼Œé€šé“ 256
                x4 = self.down3(x3)# ç©ºé—´ 1/8ï¼Œé€šé“ 512
                x5 = self.down4(x4)# ç©ºé—´ 1/16ï¼Œé€šé“ 1024/factorï¼ˆç“¶é¢ˆå±‚ï¼‰
                
                # å¦‚æœå¯ç”¨äº† Advanced_CAFMï¼Œå¯¹ç“¶é¢ˆå±‚ç‰¹å¾è¿›è¡Œå¢å¼º
                if self.use_advanced_cafm:
                    x5 = self.advanced_cafm_bottleneck(x5)
                
                # è§£ç è·¯å¾„ï¼šä¸Šé‡‡æ · + ä¸å¯¹åº”ç¼–ç å±‚çš„ç‰¹å¾å›¾æ‹¼æ¥ï¼ˆU å½¢ç»“æ„çš„"è·³è¿"ï¼‰
                x = self.up1(x5, x4)# ç”¨ x4 åšè·³è¿
                x = self.up2(x, x3) # ç”¨ x3 åšè·³è¿
                x = self.up3(x, x2)# ç”¨ x2 åšè·³è¿
                x = self.up4(x, x1)# ç”¨ x1 åšè·³è¿
                logits = self.outc(x)# 1x1 å·ç§¯ï¼šæŠŠé€šé“æ•°å˜æˆç±»åˆ«æ•°ï¼ˆæ¯ä¸ªåƒç´ è¾“å‡ºå„ç±»çš„å¾—åˆ†ï¼‰
            return logits# è¿”å›ç½‘ç»œè¾“å‡ºï¼ˆæœªç»è¿‡æ¿€æ´»ï¼›è®­ç»ƒæ—¶äº¤ç»™ Loss/åå¤„ç†ï¼‰

    def use_checkpointing(self):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šç”¨è®¡ç®—æ¢æ˜¾å­˜ï¼Œé€‚åˆæ˜¾å­˜å—é™çš„æƒ…å†µ"""
        self.checkpointing = True
        
    def disable_checkpointing(self):
        """ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šæ­£å¸¸å‰å‘ä¼ æ’­ï¼Œæ›´å¿«ä½†å ç”¨æ›´å¤šæ˜¾å­˜"""
        self.checkpointing = False