"""
dubm_module.py (Fixed Arguments)
ä¿®å¤æ—¥å¿—ï¼š
1. [å‚æ•°ä¿®æ­£] DCNv3Function.apply è¡¥å…¨äº†æ¼æ‰çš„ dilation_w å‚æ•°ã€‚
2. åŒ…å«ä¹‹å‰çš„ edge_priorã€GroupNorm å’Œ Contiguous ä¿®å¤ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# å°è¯•å¯¼å…¥ DCNv3 åº•å±‚ç®—å­å‡½æ•°
try:
    import sys
    sys.path.append("./ops_dcnv3")
    from functions.dcnv3_func import DCNv3Function
    HAS_DCN = True
except ImportError:
    HAS_DCN = False
    print("âš ï¸ D-UBM Error: DCNv3 functions not found! Please compile ops_dcnv3 first.")

class UncertaintyDCN(nn.Module):
    """
    [é­”æ”¹ç‰ˆ DCN] æ”¯æŒä¸ç¡®å®šæ€§è°ƒåˆ¶çš„ DCNv3
    """
    def __init__(self, channels, kernel_size=3, group=4, offset_scale=1.0):
        super().__init__()
        if not HAS_DCN:
            raise ImportError("Cannot initialize UncertaintyDCN: DCNv3 not compiled.")

        self.channels = channels
        self.kernel_size = kernel_size
        self.group = group
        self.offset_scale = offset_scale
        
        # 1. ä¸Šä¸‹æ–‡æå–ç½‘ç»œ (GroupNorm ä¿®å¤ç‰ˆ)
        self.dw_conv = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=(kernel_size-1)//2, groups=channels),
            nn.GroupNorm(1, channels), 
            nn.GELU()
        )

        # 2. çº¿æ€§æŠ•å½±å±‚
        self.offset_mask_linear = nn.Linear(
            channels, 
            group * kernel_size * kernel_size * 3
        )
        
        self._init_weights()

    def _init_weights(self):
        nn.init.constant_(self.offset_mask_linear.weight, 0.)
        nn.init.constant_(self.offset_mask_linear.bias, 0.)

    def forward(self, x, uncertainty_map):
        """
        x: [N, C, H, W]
        uncertainty_map: [N, 1, H, W]
        """
        N, C, H, W = x.shape
        
        # å‡†å¤‡æ•°æ®æ ¼å¼
        x_in = x.permute(0, 2, 3, 1).contiguous() 
        
        # --- A. é¢„æµ‹ Offset å’Œ Mask ---
        feat_ctx = self.dw_conv(x).permute(0, 2, 3, 1).contiguous()
        offset_mask = self.offset_mask_linear(feat_ctx)
        
        offset_dim = self.group * self.kernel_size * self.kernel_size * 2
        
        # æ‹†åˆ†å¹¶å¼ºåˆ¶è¿ç»­åŒ–
        offset = offset_mask[..., :offset_dim].contiguous()
        mask = offset_mask[..., offset_dim:].contiguous()
        
        # Mask å½’ä¸€åŒ–
        mask = mask.reshape(N, H, W, self.group, -1)
        mask = F.softmax(mask, dim=-1)
        
        # --- B. ä¸ç¡®å®šæ€§è°ƒåˆ¶ ---
        u_map = uncertainty_map.permute(0, 2, 3, 1).unsqueeze(-1)
        mask = mask * u_map 
        
        # Reshape å¹¶å¼ºåˆ¶è¿ç»­åŒ–
        mask = mask.reshape(N, H, W, -1).contiguous()

        # --- C. è°ƒç”¨åº•å±‚ç®—å­ ---
        # ğŸ”¥ğŸ”¥ğŸ”¥ [å‚æ•°ä¿®æ­£ç‚¹] ğŸ”¥ğŸ”¥ğŸ”¥
        x_out = DCNv3Function.apply(
            x_in, 
            offset, 
            mask,
            self.kernel_size, self.kernel_size,
            1, 1, # stride_h, stride_w
            (self.kernel_size-1)//2, (self.kernel_size-1)//2, # pad_h, pad_w
            1, 1, # ğŸ”¥ dilation_h, dilation_w (ä¹‹å‰æ¼äº†ä¸€ä¸ª1)
            self.group, 
            self.channels // self.group, 
            self.offset_scale,
            256, # im2col_step
            False # remove_center
        )
        
        return x_out.permute(0, 3, 1, 2).contiguous()


class DUBM_Block(nn.Module):
    """
    [D-UBM å®Œæ•´æ¨¡å—]
    æ”¯æŒåŒæµäº’è¡¥ï¼šæ¥æ”¶å¤–éƒ¨è¾¹ç¼˜å…ˆéªŒ
    """
    def __init__(self, in_channels):
        super().__init__()
        self.seg_head = nn.Conv2d(in_channels, 1, kernel_size=1)
        self.dcn_refine = UncertaintyDCN(in_channels, kernel_size=3, group=4)
        self.fusion = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

    # æ”¯æŒ edge_prior
    def forward(self, x, edge_prior=None):
        # 1. å†…æºæ€§ä¸ç¡®å®šæ€§
        p = torch.sigmoid(self.seg_head(x))
        u_self = 1 - torch.abs(2 * p - 1)
        
        # 2. äº’è¡¥èåˆé€»è¾‘
        if edge_prior is not None:
            edge_prob = torch.sigmoid(edge_prior)
            if edge_prob.shape[2:] != u_self.shape[2:]:
                edge_prob = F.interpolate(edge_prob, size=u_self.shape[2:], mode='bilinear', align_corners=True)
            u_final = torch.max(u_self, edge_prob)
        else:
            u_final = u_self
        
        # 3. DCN ç²¾ä¿®
        feat_refined = self.dcn_refine(x, u_final)
        out = x + self.fusion(feat_refined)
        
        return out, u_final